"""Respond èŠ‚ç‚¹ï¼šæ±‡æ€»å·¥å…·ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç”¨æˆ·å›å¤ï¼ˆæµå¼ï¼‰"""
from __future__ import annotations

import json
from openai import AsyncOpenAI

from config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, DEEPSEEK_MODEL
from agent.state import PassAgentState


def _build_respond_system_prompt(state: PassAgentState) -> str:
    """æ ¹æ® tool_history é•¿åº¦é€‰æ‹©å›å¤æ¨¡å¼ï¼Œæ„å»º system promptã€‚"""
    tool_count = len(state.get("tool_history", []))

    if tool_count == 0:
        mode_hint = "è¿™æ˜¯ä¸€ä¸ªé—²èŠæˆ–æ‹’ç»åœºæ™¯ï¼Œæ— å·¥å…·è°ƒç”¨ç»“æœã€‚å¦‚æœæ˜¯ä¸å£ä»¤å®‰å…¨æ— å…³çš„é—®é¢˜ï¼Œå‹å¥½åœ°å¼•å¯¼ç”¨æˆ·ä½¿ç”¨å£ä»¤ç›¸å…³åŠŸèƒ½ã€‚å¦‚æœæ˜¯æ¶æ„è¯·æ±‚ï¼Œç¤¼è²Œæ‹’ç»ã€‚å¦‚æœæ˜¯ä¿¡æ¯ä¸è¶³ï¼Œè¿½é—®ç”¨æˆ·ã€‚"
    elif tool_count <= 2:
        mode_hint = "å·¥å…·è°ƒç”¨ç»“æœè¾ƒå°‘ï¼Œè¯·ç»™å‡ºç®€çŸ­ç²¾ç‚¼çš„å›å¤ã€‚"
    else:
        mode_hint = "å·¥å…·è°ƒç”¨ç»“æœè¾ƒå¤šï¼Œè¯·ç»™å‡ºè¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚"

    return f"""\
ä½ æ˜¯ PassAgentï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å£ä»¤å®‰å…¨æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®å·¥å…·è°ƒç”¨ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤ã€‚

## å›å¤è¦æ±‚
- ç”¨ä¸­æ–‡å›å¤ï¼Œä¿æŒä¸“ä¸šä¸”å‹å¥½çš„è¯­æ°”
- {mode_hint}
- åœ¨å›å¤æœ«å°¾è‡ªç„¶åœ°é™„å¸¦ 2-3 ä¸ªå¼•å¯¼æ€§å»ºè®®ï¼ˆä½œä¸ºå›å¤æ–‡æœ¬çš„ä¸€éƒ¨åˆ†ï¼Œä¸è¦å•ç‹¬ç»“æ„åŒ–è¾“å‡ºï¼‰
- å¼•å¯¼å»ºè®®ç”¨æ¢è¡Œå’Œ emoji å‰ç¼€ï¼Œä¾‹å¦‚ï¼š
  - ğŸ” æŸ¥çœ‹è¿™ä¸ªå¯†ç æ˜¯å¦æ³„éœ²
  - ğŸ”‘ å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªæ›´å®‰å…¨çš„å¯†ç 
- ä¸è¦æš´éœ²å†…éƒ¨å·¥å…·åç§°ï¼Œç”¨è‡ªç„¶è¯­è¨€æè¿°åˆ†æè¿‡ç¨‹
- å¦‚æœæ¶‰åŠå¯†ç å¼ºåº¦è¯„åˆ†ï¼Œç”¨ç›´è§‚çš„æ–¹å¼è¡¨è¾¾ï¼ˆå¦‚ "è¯„åˆ† 1/4ï¼Œè¾ƒå¼±"ï¼‰"""


def _build_tool_results_message(state: PassAgentState) -> str:
    """å°† tool_history æ ¼å¼åŒ–ä¸º LLM å¯è¯»çš„ä¸Šä¸‹æ–‡ã€‚"""
    if not state.get("tool_history"):
        return ""

    parts = ["ä»¥ä¸‹æ˜¯æœ¬è½®å·¥å…·è°ƒç”¨ç»“æœï¼š\n"]
    for i, t in enumerate(state["tool_history"], 1):
        tool_name = t["tool_name"]
        params = json.dumps(t.get("params", {}), ensure_ascii=False)
        result = json.dumps(t.get("result", {}), ensure_ascii=False)
        parts.append(f"{i}. [{tool_name}] å‚æ•°: {params}\n   ç»“æœ: {result}\n")

    # è®°å¿†ä¸Šä¸‹æ–‡
    if state.get("memories"):
        parts.append("\nç”¨æˆ·è®°å¿†ï¼š")
        for mem in state["memories"]:
            parts.append(f"  - [{mem.get('memory_type', '')}] {mem.get('content', '')}")

    return "\n".join(parts)


async def respond_node(state: PassAgentState) -> dict:
    """Respond èŠ‚ç‚¹ï¼šæµå¼ç”Ÿæˆæœ€ç»ˆå›å¤ã€‚

    é€šè¿‡ state ä¸­æ³¨å…¥çš„ event_queue å°† response_chunk äº‹ä»¶æ¨é€ç»™ SSEã€‚
    è¿”å›å¯¹ state çš„ partial updateï¼Œå°†å®Œæ•´å›å¤è¿½åŠ åˆ° messagesã€‚
    """
    client = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)
    event_queue = state.get("_event_queue")  # è¿è¡Œæ—¶æ³¨å…¥ï¼Œä¸å±äº TypedDict

    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": _build_respond_system_prompt(state)}]

    # å¯¹è¯å†å²
    for msg in state["messages"]:
        if hasattr(msg, "type"):
            role = "user" if msg.type == "human" else "assistant"
        else:
            role = msg.get("role", "user")
        content = msg.content if hasattr(msg, "content") else msg.get("content", "")
        messages.append({"role": role, "content": content})

    # å·¥å…·ç»“æœä¸Šä¸‹æ–‡
    tool_context = _build_tool_results_message(state)
    if tool_context:
        messages.append({"role": "system", "content": tool_context})

    # æµå¼è°ƒç”¨ LLM
    stream = await client.chat.completions.create(
        model=DEEPSEEK_MODEL,
        messages=messages,
        stream=True,
    )

    full_content = ""
    async for chunk in stream:
        if chunk.choices and chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            full_content += content
            # æ¨é€ SSE äº‹ä»¶
            if event_queue is not None:
                await event_queue.put({
                    "event": "response_chunk",
                    "data": {"content": content},
                })

    # å°†å®Œæ•´å›å¤ä½œä¸º AIMessage è¿½åŠ åˆ° messages
    from langchain_core.messages import AIMessage
    return {
        "messages": [AIMessage(content=full_content)],
        "next_action": None,
    }

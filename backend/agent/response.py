"""Respond èŠ‚ç‚¹ï¼šæ±‡æ€»å·¥å…·ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç”¨æˆ·å›å¤ï¼ˆæµå¼ï¼‰"""
from __future__ import annotations

import json
import logging
from openai import AsyncOpenAI
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage

from config import LLM_API_KEY, LLM_BASE_URL, LLM_MODEL
from agent.state import PassAgentState

logger = logging.getLogger(__name__)

def _build_respond_system_prompt(state: PassAgentState) -> str:
    """æ„å»ºå¢å¼ºç‰ˆ System Promptï¼Œå¼ºåŒ–ä¸“å®¶äººè®¾å’Œè¾“å‡ºç»“æ„ã€‚"""
    tool_history = state.get("tool_history", [])
    tool_count = len(tool_history)
    
    # åŸºç¡€äººè®¾
    base_persona = """
ä½ å« PassAgentï¼Œæ˜¯ç”¨æˆ·çš„**ä¸ªäººå£ä»¤å®‰å…¨å®¡è®¡ä¸“å®¶**ã€‚ä½ çš„æ ¸å¿ƒèŒè´£æ˜¯è¯„ä¼°é£é™©ã€å‘ç°éšæ‚£å¹¶æä¾›åŠ å›ºå»ºè®®ã€‚
ä½ çš„å›ç­”å¿…é¡»ï¼š
1. **å‡†ç¡®ä¸¥è°¨**ï¼šåŸºäºå·¥å…·è¿”å›çš„æ•°æ®è¯´è¯ï¼Œä¸è¦ç¼–é€ æœªæ£€æµ‹åˆ°çš„é£é™©ã€‚
2. **é€šä¿—æ˜“æ‡‚**ï¼šå°†æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚"å“ˆå¸Œç¢°æ’"ã€"ç†µå€¼"ï¼‰è½¬åŒ–ä¸ºç”¨æˆ·èƒ½æ‡‚çš„è¯­è¨€ã€‚
3. **å®‰å…¨ç¬¬ä¸€**ï¼šå¦‚æœå·¥å…·è¿”å›äº†æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚æ˜æ–‡å¯†ç ï¼‰ï¼Œåœ¨å›å¤ä¸­åº”è¿›è¡Œæ‰“ç å¤„ç†ï¼ˆå¦‚ `P***d`ï¼‰ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚æ˜¾ç¤ºã€‚
"""

    # åŠ¨æ€ä»»åŠ¡æŒ‡ä»¤
    if tool_count == 0:
        task_instruction = """
å½“å‰çŠ¶æ€ï¼š**é—²èŠæˆ–æ„å›¾è¯†åˆ«é˜¶æ®µ**
- å¦‚æœç”¨æˆ·æ˜¯åœ¨æ‰“æ‹›å‘¼ï¼Œè¯·çƒ­æƒ…å›åº”å¹¶ç®€è¿°ä½ èƒ½åšä»€ä¹ˆï¼ˆå¦‚ï¼šæ£€æµ‹å¯†ç å¼ºåº¦ã€ç”ŸæˆæŠ—ç ´è§£è§„åˆ™ã€æŸ¥è¯¢æ³„éœ²åº“ï¼‰ã€‚
- å¦‚æœç”¨æˆ·çš„é—®é¢˜è¶…å‡ºäº†"å£ä»¤å®‰å…¨"èŒƒç•´ï¼Œè¯·ç¤¼è²Œåœ°å°†è¯é¢˜å¼•å¯¼å›ä½ çš„ä¸“ä¸šé¢†åŸŸã€‚
- æ‹’ç»å¤„ç†ä»»ä½•éæ³•çš„ç ´è§£è¯·æ±‚ï¼ˆå¦‚"å¸®æˆ‘ç ´è§£éš”å£çš„WiFi"ï¼‰ã€‚
"""
    else:
        task_instruction = """
å½“å‰çŠ¶æ€ï¼š**åˆ†ææŠ¥å‘Šç”Ÿæˆé˜¶æ®µ**
è¯·æ ¹æ®ä¸‹æ–¹çš„ `<tool_outputs>` ç”Ÿæˆå›å¤ã€‚éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š

### 1. æ ¸å¿ƒç»“è®º
ç”¨ä¸€å¥è¯æ¦‚æ‹¬ç»“æœï¼ˆä¾‹å¦‚ï¼š"æ£€æµ‹é€šè¿‡ï¼Œæ‚¨çš„å¯†ç å¼ºåº¦æé«˜" æˆ– "è­¦å‘Šï¼šå‘ç°è¯¥å¯†ç åœ¨3ä¸ªæ³„éœ²åº“ä¸­å‡ºç°"ï¼‰ã€‚

### 2. è¯¦ç»†åˆ†æ
- è§£è¯»å·¥å…·è¿”å›çš„æ•°æ®ï¼Œä¸è¦ç›´æ¥ç½—åˆ— JSON å­—æ®µã€‚
- å¦‚æœæ¶‰åŠè¯„åˆ†ï¼Œè¯·ç”¨ç›´è§‚æè¿°ï¼ˆå¦‚ ğŸ”´é«˜å±ã€ğŸŸ¡ä¸­ç­‰ã€ğŸŸ¢å®‰å…¨ï¼‰ã€‚
- è§£é‡Šä¸ºä»€ä¹ˆä¼šå¾—å‡ºè¿™ä¸ªç»“è®ºï¼ˆä¾‹å¦‚ï¼š"å› ä¸ºå®ƒç”±çº¯æ•°å­—ç»„æˆ"ï¼‰ã€‚

### 3. åç»­å»ºè®®
- é’ˆå¯¹å½“å‰æƒ…å†µç»™å‡º 2-3 æ¡å…·ä½“è¡ŒåŠ¨å»ºè®®ã€‚
- å»ºè®®å¿…é¡»å…·æœ‰å¯æ“ä½œæ€§ã€‚
"""

    return f"{base_persona}\n{task_instruction}"


def _build_tool_context(state: PassAgentState) -> str:
    """å°†å·¥å…·ç»“æœå’Œè®°å¿†æ ¼å¼åŒ–ä¸ºç»“æ„æ¸…æ™°çš„ XML ä¸Šä¸‹æ–‡ï¼Œä¾¿äº Qwen ç†è§£ã€‚"""
    
    context_parts = []

    # 1. å¤„ç†å·¥å…·è°ƒç”¨å†å²
    if state.get("tool_history"):
        tools_str = []
        for i, t in enumerate(state["tool_history"], 1):
            tool_name = t["tool_name"]
            # ç®€åŒ– resultï¼Œé˜²æ­¢è¿‡é•¿ JSON æ’‘çˆ†ä¸Šä¸‹æ–‡
            result_str = json.dumps(t.get("result", {}), ensure_ascii=False)
            status = "æˆåŠŸ" if t.get("status") != "error" else "å¤±è´¥"
            
            tools_str.append(f"""
<tool_execution id="{i}">
    <name>{tool_name}</name>
    <status>{status}</status>
    <result>{result_str}</result>
</tool_execution>""")
        
        context_parts.append("<tool_outputs>\n" + "\n".join(tools_str) + "\n</tool_outputs>")

    # 2. å¤„ç†é•¿æœŸè®°å¿† (User Profile)
    if state.get("memories"):
        mem_str = []
        for mem in state["memories"]:
            m_type = mem.get('memory_type', 'INFO')
            content = mem.get('content', '')
            mem_str.append(f"- [{m_type}] {content}")
        
        context_parts.append("<user_profile>\n" + "\n".join(mem_str) + "\n</user_profile>")

    return "\n\n".join(context_parts)


async def respond_node(state: PassAgentState) -> dict:
    """Respond èŠ‚ç‚¹ï¼šæµå¼ç”Ÿæˆæœ€ç»ˆå›å¤ã€‚"""
    client = AsyncOpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
    event_queue = state.get("_event_queue")

    # 1. æ„å»º System Prompt
    system_content = _build_respond_system_prompt(state)
    
    # 2. æ„å»ºä¸Šä¸‹æ–‡æ•°æ® (å·¥å…·ç»“æœ + è®°å¿†)
    context_content = _build_tool_context(state)
    
    # 3. ç»„è£… Messages
    # è¿™é‡Œçš„æŠ€å·§æ˜¯ï¼šæŠŠ System Prompt æ”¾åœ¨æœ€å‰ï¼ŒæŠŠå·¥å…·æ•°æ®ä½œä¸º System Message ç´§éšå…¶å
    # æˆ–è€…ä½œä¸º User Message çš„è¡¥å……ã€‚å¯¹äº Qwenï¼Œåˆ†å¼€æ”¾ System æ•ˆæœè¾ƒå¥½ã€‚
    messages = [
        {"role": "system", "content": system_content},
    ]

    # å¦‚æœæœ‰å·¥å…·ä¸Šä¸‹æ–‡ï¼Œä½œä¸ºè¾…åŠ© System ä¿¡æ¯æ’å…¥
    if context_content:
        messages.append({
            "role": "system", 
            "content": f"è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡æ•°æ®å›ç­”ç”¨æˆ·ï¼š\n{context_content}"
        })

    # è¿½åŠ å†å²å¯¹è¯
    for msg in state["messages"]:
        if hasattr(msg, "type"):
            role = "user" if msg.type == "human" else "assistant"
        else:
            role = msg.get("role", "user")
        content = msg.content if hasattr(msg, "content") else str(msg.get("content", ""))
        messages.append({"role": role, "content": content})

    # æµå¼è°ƒç”¨ LLM
    try:
        stream = await client.chat.completions.create(
            model=LLM_MODEL,
            messages=messages,
            stream=True,
            temperature=0.7, # ç¨å¾®é™ä½æ¸©åº¦ï¼Œä¿è¯åˆ†æçš„ä¸¥è°¨æ€§
            max_tokens=2048,
        )

        full_content = ""
        async for chunk in stream:
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta
            # å…¼å®¹ DeepSeek/Qwen çš„ä¸åŒå­—æ®µ
            content = delta.content or getattr(delta, "reasoning_content", None) or ""
            
            if content:
                full_content += content
                if event_queue is not None:
                    await event_queue.put({
                        "event": "response_chunk",
                        "data": {"content": content},
                    })
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
        full_content = "æŠ±æ­‰ï¼Œæˆ‘çš„å¤§è„‘æš‚æ—¶çŸ­è·¯äº†ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚"
        if event_queue:
            await event_queue.put({"event": "response_chunk", "data": {"content": full_content}})

    if not full_content:
        full_content = "ï¼ˆæœªç”Ÿæˆä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥å·¥å…·è¾“å‡ºæ˜¯å¦è¿‡é•¿å¯¼è‡´æˆªæ–­ï¼‰"

    return {
        "messages": [AIMessage(content=full_content)],
        "next_action": None,
    }
首先是背景这一块
口令复用现象普遍，然后一方面是讲hashcat这种静态规则库局限、容易复杂度爆炸；一方面讲其他的口令猜测的模型

主要是把模型的架构讲清楚


实验这一块：
+ 不同训练集大小的影响（正在做）
+ 对于qwen3模型而言，不同参数量的影响（已做）
+ 对于llama模型而言，不同参数量的影响（未做）和qwen的放一起对比
+ 训练不同学习率的影响（针对Qwen3 1.7b），要补充
+ 训练采用的提示词（针对Qwen3 1.7b），要补充
+ 撞库实验，验证即使多口令也有用，要补充
+ 如果有空再补一个跨站攻击，不过不必须
+ 或许还有不同生成方式的影响？
+ 对了还有其他模型的对比，目前能做的是Pass2path、Targauss/PCFG?OMEN? or passgpt https://github.com/javirandor/passgpt

A: PCFG目前的做法是定义的一些……实际上是建一堆Dictionary，然后底层是用了一个英文的语法解析器，再就是加了一堆Dictionary。这个做法其实并没有真正理解非英语，然后另外就是它语义其实还是比较少的。虽然我们已经做了几十种，但是还是不够的。OK，你继续吧，反正你大概看过这篇文章是吧？
B: 嗯，你看过，看过，对。所以你大概能理解我说的事情是吧？
A: 嗯，对对对。OK，那你继续。
B: 嗯，好。呃，然后对于我们的这个，因为我是通过对大模型做微调，去……让他让这个模型去具备……就是我输入一个原始的口令，然后他能够给我说出适合这个原始口令的规则变化。就比如说是这样的一个数据集，就是我会给他一个原始的口令、目标的口令，以及它生成的一个规则的一个方式。然后数据集的来源的话呢，这边是采用的这6个泄露的密码库，然后呢是采用的一个……就是邮箱匹配，去匹配出了一共有大概有200多万个用户的一个复用的情况。
B: 然后在这个里面的话，为了去生成这里的Generate Rule，我们是用了一个基于A星算法的……就是把这个过程给它建模成了一个最短路径的一个问题。然后呢，最后是采用A星算法去批量地生成这个规则。然后这个……我们这这个算法的话呢，它大概是有90%的一个精度。
B: 然后最后这一块的话，还是做了一些就是数据清洗的工作。一个是把其中只保留了英文、数字、符号，这些就只保留了ASCII码。然后的话呢，把这里面的一些……就是基于位置坐标的，就比如说这里的S21（把第二个字符替换成什么什么），像这种规则的话呢，就是把它去除掉。因为我们觉得这种基于位置坐标的替换的话，它……就是即使对于LLM去学习它的一个语义信息是没有多大的帮助的。
B: 然后还有去除的一些比较长的规则，当然我们是去除了大于5个以上的这种规则。以及就是对于总体的一个进行的一个统计量的分析，就是把出现次数小于3次的这些都也都去除掉。
A: 那个我想问一下，你说的那个规则到底是啥规则？你说的就是HashCat的规则啊？
B: 对，就是HashCat的规则。
A: 是一个非Low Target……就说是所有的这个密码结构里边最不智能的一个密码结构。因为我这边就是……就是想要找到一种方式去描述它的变化嘛。我的意思就是说，如果用SR-PCFG的话，会比要智能多，就是你就知道到底……就是它是描述的密码的真正结构的嘛。这个HashCat是盖的应该没有那么……它涵盖就主要是些转换对吧？
B: 嗯，对对。
A: 你如果只是做HashCat的话，我估计发文上应该非常难。因为HashCat其实只是一个程序而已，它都谈不上算法，HashCat本身都没有嘛。
B: 嗯，呃，对。就是我这边的这个HashCat，它其实就是作为一个……就是描述的方式吧。就是比如说像那个他们的Pass2Pass或者PaEdit，他们其实采用的也就是把那个每一个删除、替换、插入啊给他……就是他们并不是采用HashCat的方式，就是采用也是把他们定义成一种……一个单元吧，就是模型输出的单元。
A: 它描述的是变化，就是说他们做法是描述的是变换规则。如果是用……如果是用SR-PCFG或者是其他的带Semantic的这个规则的话，它那个做法就是是两步走。你先把老的密码切成这个……表示成密码的结构，密码的那个上方的语义结构；再把新的密码表示成语义结构；然后再这这两个结构再做比较，再找出来它的变化。你们做的是一步，其实那种应该是两步来做的。
C: 啊，好的。其实是这样的，就是那个我们目前……就是那个张林浩同学，他现在在做的其实就是还是那种那个一步式的。就是因为比如说那个Password，它可能是那个A变成@呀，然后那个S变成Dollar啊，就这种。就他这个我们目前做的对的还是这种变换的规则。然后我们这里他其实并没有……还没有做到就是完全语义层面的，这种就是语义层面的话，我们可能也是会放到后面，我们可再去看一看要不要去做这一块。
A: 对，如果是变换的话，有个最大的问题，就是说其实SR-PCFG已经部分Cover这个用例了嘛。我们只不过……对对，之前做的话，应该说做的还是做的不是那么好的，但是做是做是肯定做了的，对。那么前一阵子，就是之前我和那个……就那个博士生，他已经走了，还有那个袁海月，在尝试第一部分的时候，其实基本上把这部分的一些粗糙的实验已经做了。就是说，Learn Transformation LM是完全可以复现的。就是你现在这个任务其实我们已经做过了，只不过是在从一个密码上做的。但是我们做这件事情呢，其实是这样：一个已经被转换过的密码，我们会说你把原来的密码给我找出来，就是原来没有转换之前的密码，然后把这个转换规则给我列出来。就这件事情，这点其实就是……其实感觉就是这样一套做的事情，比他做事之前还复杂，因为我们要他还原之前的密码。
C: 嗯，对。这个其实就是说我们……就是我们其实那个叫张林浩刚才其实没太那个讲，就是我们做这块的目的是什么。我们做这块的目的是呢，其实是针对就是现在的这个变换规则。它其实是很多情况下，他比如HashCat也好，或者说那个其他比如说那个PassGAN也好，他们做的呢其实很多情况下是，比如说他会去生成一个这个Rule Set的列表。然后呢，他去测性能的时候呢，他是给定一个口令集，然后呢，他去和这个规则集，他去做一个笛卡尔积。然后呢，他去这个……就相当于是每一条规则都应用在每一个口令上。然后呢，他去做一下那个……就是这样的话，就是那个GPU会生成出很多的这些新的口令嘛，然后他去攻一个这个目标集，看看能攻的这个效果如何。就这个是以前的那个论文是都是这么做的。
C: 然后我们其实这个想法呢，其实就是有点像是……我想用是否找到一种途径或者一种方法，我是否是能根据那个就是已经有的这些，比如说张林浩现在列出来这个，甚至说可能包括那个李教授您做已经做了这个实验，我们也可以这个……引入进来。说我们希望能是我能通过一些这个方式方法，我们能去判断一个：当我给定某一个口令的时候，我想针对这一个口令去生成出针对这个口令的……
A: 比如说这个Top K的这样的一个规则集，这样的话呢就是相当于说，针对某一个口令，它有一个针对它的专用的规则集。我就可能会避免，比如说我一个口令，他可能会用到不属于它，或者说他其实并不能完全适配的这种规则了。就是我们是想通过这种方式来去减少那种就是完全笛卡尔积的这种消耗量，但是同时呢我们也能够达到比较好的攻击效果。这个其实是张立浩做目前的这个研究的一个主要的目的。
B: 对，就是说我的想法就是说，如果你... 就是说之前的工作他们没有人做过这个两个之间的规则变化，从中来学习他这个规则变化，然后再应用于做Cracking，没有人做过这样的思路，是吗？
A: 呃，应该说是也有人去做过这个。但是呢就是比如说那个之前Pass2Edit呀，或者Pass2Pass这种，他们也做过一些。但是他们做的这个就是可能我们整体来讲，从方法论上来说可能是不太一样的，但是目标应该是一致的。就是我们思路上应该是... 我们的目标应该是一致的，就是相当于是我都是想去找到一些更好的规则来去应用在这个口令上，但是可能用的这种方式方法是不太一样。
B: 就说他们的方法是什么呢？这里用的是A*嘛，你这用的是... 这个张立浩，那个你还记得吗？
A: 啊，这个的话我这边这个A*这个算法的话，它只是用来生成数据集的，并不是我的模型是长成这个样子。然后对于那个Pass2Pass、Pass2Edit的话，他们基本都是采用的就是自己去构建的一个RNN的模型，就是他们顺着RNN的模型去推测Rule的变化。
B: 我看看你说的这个A*是 Rule extract from... 好的。哎，你说的这个D是什么？你怎么没听懂，你再说一遍。
A: 呃，因为我这边不是就是我要微调我的大模型嘛，就是让大模型具备我输入一个口令，然后输出的是适合这个口令的规则集。然后为了实现这个目标，所以我就是要生成这边这种数据集嘛。所以我... 然后生成数据集的时候，因为我目前的数据是只有它的Pass Old和Pass New，所以我如何就是采用A*算法呢，它是为了去生成这个Generated Rule。
B: 哦，我还是没听懂。你刚才说的是用来是用数据集的，现在是变成生成Rule的，是什么意思？
A: 呃，就是您看，就是右下角这个就是我的数据集，就是长成这个样子。然后我在训练大模型的时候啊，就是我的输入就是我的Original的口令，然后输出呢就是我这边的Generated Rule。然后因为我要训练它，所以我要有数据集嘛。我的数据集就是我之前不是通过那个邮箱匹配去拿到了所有的Original和Target的密码Pairs嘛，然后那我一定也要知道他们的Generated Rule。那这边这个Generated Rule我是通过就是A*算法去获得的。
B: 就是说你是用这个来生成Ground Truth，这意思是啊？
A: 对对对对。
B: 就是这个Ground Truth是和文献里面是什么区别？那现在是... 怎么不让出去的？
A: 呃，您是说跟那个Pass2Pass他们相比吗？
B: 就所有的... 所有文献里面，他们也需要Picture是吗？一样道理，对吧？他们... 嗯，他们应该没有在那个里面设置就是自己的数据集怎么去清洗。他如果没有保证的话，就说你的意思是他是没有... 他是没有Pattern Detection的这个的，只有Prediction，是这意思是吧？
A: 呃，嗯，怎么说就是... 他这边怎么去判断他的话，怎么去判断他是不是正确的，基本就是我直接去相当于去用HashCat嘛。然后作用在Original的Password上，看看能不能变成他给他的Password。
B: 嗯，就有HashCat嘛，对吧。所以就是说之前所有文章的思路都是都没有这一步，就没有这个Ground Truth这一步，没有Ground Truth Check这一步。它是有一个算法，预测了这个Rule的变化，然后直接上Cracking，然后根据Cracked来间接证明他的这个方法是正确的，是这意思？所以他不需要Ground Truth，是这意思吧？
A: 嗯，对。他们有的方法就是直接去用了HashCat的一个... 就因为HashCat它是有那个规则集的嘛。然后比如说他那个规则子里面就是有5万种规则，然后他是就是把这5万种规则当成了一个分类问题去做，就是从里面去选。
B: 还有一种方式就是去... 应该也是采用分类解决个分类问题，那你还仍然需要Ground Truth嘛。如果要是判断分类器的性能，你仍然需要Ground Truth。就是它的Ground Truth怎么了？还是说他没有Ground Truth？我就现在只是想了解这个，因为这方面的文献我读的比较少。我对这个之前... 对这个Password Cracking这块我一点都不感兴趣，我其实我现在也不是很感兴趣，因为我觉得这是一个非常Narrow的问题。现实中的问题，现实中的影响但是比较大，有一定影响，但其实影响不是那么大。呃，不管这个... 从方向角度没有关系。就是说你能不能确认一下，根据你的文献的阅读，他们确实都是没有Ground Truth这一步的，所以他们的那个Pattern Accuracy只能通过Password Cracking的Performance来间接证明？
A: 呃，嗯，是的是的是的，是这个意思。
B: 那就是说你现在做的工作是把这个Gap补上了是这意思吗？
A: 嗯，应该说我，我的这种也是就是... 就是我具体在评估这个模型性能的时候，我相当于我也是通过我去Crack以后看他对不对这样。
B: 也就说其实你也是... 不是True这一步，是这样的？
A: 呃...
B: 那你是说什么OK？
A: 呃，对，这个是这样的。就是那个其实他们Pass2Pass也好，Pass2Edit也好，就是刚才我说的就是，其实他们呢也是有一个类似这样的一个步骤的，就相当于说他们有一个这个Prediction之后，然后呢他们是那个用哪些步骤...
A: 然后呢，比如说去生成一些具体的规则，从原始的那个口令变换成一个新的口令，这一步其实是都有的。只不过这一步确实我记不太清楚，他们到底用的是什么样的一个方法来去做的这一步了。然后呢，我们用的是这个 A*（A-Star）的算法来实现。就是当我拿到一个 Pair 之后，我去从这个 Pair 去找到我需要用到了哪些规则。其实这一块的话呢，也可以应用那个就是李教授刚才您说的那个 LLM（大模型）的方法，就是当我有了这个 Pair 之后，我就去扔给大模型，然后让他去帮我分析我用了哪些规则，我就可以进行这个变化。这一步因为他们应该是都有的，因为他们只有这一步相当于说可以认为是 Ground Truth，但是也可以认为我们其实就是做了一个训练集。就这个可能是我们看待它的方式不太一样。因为如果做 Ground Truth 的话，比如说我们在用模型学习完之后，一种方式是其实和我的 Ground Truth 进行比较，也就是说我通过模型之后，我会去生成我这个变化步骤。那这个是一种方式。但是实际上来说，对于文献而言，目前文献上他们并不是直接和这个所谓的 Ground Truth 进行比较的。他们在评估这个模型性能的时候，他们是直接去把模型输出的结果去应用到一个训练集上，或者应用到一个口令集上，然后去生成一个新的口令集，然后去看一下这个新的口令集去攻击另外一个库，它的效果如何。就是大家可能都有这一步，但是并没有把它当做这个 Ground Truth 来进行对待，我觉得可能是这样的一个情况。但其实就是他就是用了 P_old...

B: 对对对，我再讲一下我们目前的理解啊，我们目前理解就是这样。为什么别人不需要 Ground Truth？原因就是因为从 P_old 到 P_new 到底是怎么变过去的，Nobody cares。
A: 是，对。
B: 然后呢，这是一点。另外一点是实际上是没有 Ground Truth 的，我告诉你，这是我有点 Concern 的地方。就说当你用了一个所谓的 A* 找到了一个 Ground Truth，用这个 Ground Truth 训一个... 其实是不是就是一个 LLM 的时候，你其实是 Wrong 的。因为我们在做第一部分的时候，包括做 list 那个 Transformation 的 Recovery，包括做其他的，不做例子也发现这个问题。其实之前我和阿德跟你讲过这个问题，就是说一个 Password 生成的，哪怕在没有 Pair 的情况里边，哪怕一个 Single Password 的情况也是一样，这个 Transformation Pattern 和每一个 Password 的 Internal Pattern 都有同样的问题，叫做多义性（Ambiguity）。
B: 这个在 Single Password 的情况下，就是说这个 Password 的 Semantic Segmentation 不是只有一种的，是有好几种分割法的。不是所有的密码都这样，有的密码是只有一种，有的密码是有多种。对多种分割法中的哪一个是 Ground Truth？No，是这样。You can estimate/predict potential possibilities。比如说一个密码的 Semantic Pattern，里边的三种里边可能有一个是 95%，就是因为里面有些东西是高概率，可能很多人都会用的；但是剩下两种可能是 2%、3%，虽然他不太可能被用但是有些人还是会用。
B: 当然还会有更复杂的情况，非常类似的，你这个 Pair 的情况也是一样的。完全一样的规则，从 P_old 到 P_new，虽然是说很多情况下，他只有一种我们人能够看得到的转换模式，可能也是真实的那个这个人当时转换的模式，但实际上在你不知道那个人的真实意图的情况里，他其实是可能有好几种转换模式的。这个有不同的所谓... 如果是用 Path To 那种思路的话，他其实也就是说有 multiple edge paths, and every path is legitimate。你如果用 A* 的话，你用的其实是 Shortest Path 对吧？
A: 对，可以这么理解。
B: 但是 Shortest Path 不一定是用户当时脑子里想的那个 Path。
A: 对。
B: 然后呢就变成了就是说你用 A* 找到了这个所谓的 Ground Truth，这个 Pattern 其实不一定是 Ground Truth Pattern。因为然后你还要意识到，一般的他在做这个变换的时候，他一般的 Change 其实并不是非常多，一般就是一个、两个、三个，估计超过三个都比较少。我不知道你们做过这个统计没有。那么在这种情况底下的话，这种 Multi-path 这种 Confusion 可能概率会比较高。就是说你算出来的可能是 One，但其实这个用户用的是 Two。
B: 取决于具体情况，但是现在不看具体的例子不太容易讲。那么也有可能你假设是 2，实际上他们是 3，因为他之间本来就差没差多少，对吧？取决于他当时原始密码的 Semantic Segmentation 是怎么样子。就是如果说他原来比如说有 5 个 Segment，你不知道有 5 个 Segment，对吧？一看可能有两个变化，但其实他那个 5 个 Segment 决定了——你如果知道这 5 个 Segment 再去看它的变化，你就会发现在这直接这么两个做其实不太合理的，因为它不是那么回事。它其实是在那 5 个 Segment 里边对两个做了变化，然后又扔掉了一个东西，然后就变成了这样一个东西。你认为是这样了，对吧？
B: 但是说至一说有多少个，有多大概率，就你做的 A* 有多大概率是正确的反应他那个 Mental... Manifold 里边的那个正确的那个？这个我们是不知道的，就是没有那个 Ground Truth，对吧？
A: 对。
B: 这是一个部分，就是说关于这个 Multi-path 的这个问题，这个所谓的多义性。多义性的话其实是非常有意思的，而且之前那个 C2R/PCFG 也是没做的。那么多义性实际上对我们后面做 Smoothing 也是非常重要。因为加多义性，我们才可以更大的扩充你的 Generalizability，对吧？因为你观察到的不代表是唯一的一种变化，还有其他变化，对吧？那么这是一个大的点。另外一个点，其实和你这个用 Ground Truth 的一个出发点有关。如果我现在理解不错的话，你之所以要用 A* 找一个 Ground Truth，而不是直接就找到任何一个能够从 P_old 到 P_new...
A: 然后不做任何 Check，你为什么、为什么别人不做 Check 呢？就是因为 New 就是 Ground Truth 呀。你要什么签下去呀？你只要能找到一个 Parse，能够从 All... 就它就是 Ground Truth。Who cares？因为你不知道 Ground Truth 是谁，对吧？只要你能够预测 New，你就是正确的，没有什么 Truth 的问题。所以这对他们来讲，这里根本就没有一个 Ground Truth 的问题，因为 New 就是 Ground Truth，对吧。那么你现在人为地造出来的 Ground Truth 的问题是什么呢？是因为你要做 Fine-tuning，对吧？
B: 嗯，是的是的。
A: 你要做，但是呢你做的 Fine-tuning 变成了是把你的 P-New 作为 Ground Truth，转换成了一个从 PD6 到 P-New 里边的、M-Path 里边的其中的一个 Path 作为你的 Ground Truth。就是呃，这是画蛇添足。说实话就说实际上是不需要的，也会 In-context（inin sp）... 这是一个层面。但是呢还有一个其实更重要的层面，就是其实你是不需要做 Fine-tuning 的。我现在不清楚你、你、你现在做了工作，就是做完 Fine-tuning 了之后，你认为你的性能提升了多少或者怎么样？不知道你做没做那个 Zero-shot 那个... 呃，Zero-shot 和这个 Few-shot 就是做 Diff 这种一个性能的比较。我很怀疑你的 Fine-tuning 中有任何的性能提升，这个你是测试了没，那个小友啊？
B: 有的有的，它提升有多大... 呃，它提升的话就是... 我翻到那个... 它提升的话就是还是比较大的。我翻到最后那边吧，就是... 呃，就是当时是跟那个 Parse-to-Parse、Passport 的一个比较。呃，就是目前是这样的一个情况，就是我是把它的验证集分成了... 呃只有一种变化的，以及大于一种... 就是这个是做 Correction 的结果，啊对，Correction 的结果。
A: 嗯，然后哪一个是 Zero-shot，哪一个是 Few-shot，哪一个是 Fine-tuning？
B: 呃，这边... 呃这边就是那个呃 Fine-tuning 后结果的那个那个图嘛。然后呃这三个图的话，它是就是不... 在不同的验证集上做的一个测试。
A: 哦你没听懂我的意思。我的意思是你有没有做 Zero-shot 的那个 LLM 的结果，有没有那两线？那... 那两线和你的这个的线的差异？
B: 呃，这个是这样的，李教授。就是这个呢，我们的那个方法呢，实际上是呃... 不太有 Zero-shot 的这个这个概念的。因为呢我们的这个做... 呃这个这个 Fine-tuning 的这个方法，实际上是... 就是我是用那个 Transformer 的那个呃库，相当于说我又重新去做了一个这个微调训练。那这个如果是做微调训练的话，它肯定是得有一些这个数据基础的吧。就是... 所以说我...
A: 其实你用的不是 LLM 是吧？
B: 因为的就是一个呃... 我们是用了 LLM 的，然后我们继续对这个 LLM 的模型进行了一个微调。就是呃... 呃但是我们用这个微调，我们不适合它进行比如说 Chat 的这种方式来进行这个这个... 让他去去呃... 去输出我们想要的那个内容。呃，我们相当于是对他去进行... 对对这个模型进行了一个相当于是后训练，就是这种方式对这个模型进行这个呃微调的。呃，这种方式其实那个就是汪定的那个，就是 2025 年发的那个就是 ParseLM，他用的也是这种方式。就是他用的那个微调方式，不是说和我直接和大模型进行这个 Chat 的这种这个交互，他是找到了一个基体模型之后，然后呢再... 那个有些这个新的数据添加进去，然后呢... 有些这个相当于是我去做了一个这个重新去做了一个这个微调训练。
A: 对，反正你不管怎么说，就是你其实还是做的那个 Fine-tuning 嘛，对吧？
B: 对对对对对。我的想法就是说... 对。
B: 但是就是那个 Zero-shot、Few-shot 它是针对 Prompt Learning 的嘛，就是那个提示词工程的嘛。所以说就是我们这里面做的其实就没有做提示词这一块，也就没有那个 Zero-shot 的这一块。那个所以就是... 所以就是... 所以就是没有。
A: 就是说我要做一件事情，实际上就是说如果你做了那个 Few-shot，如果呃是 Zero-shot，如果说是能做顺的话，你就会发现我... 我的预测... 我的预测是你的 Few-shot 的那个... 就是 Zero-shot 那个结果。你这个蓝线可能是你们的那个现在做下来的这结果，对吧？
B: 啊，是的是的，对。
A: 然后这个绿线和红线是 Parse-to-Parse 和 ParseBERT 还是什么东西？
B: 呃，是一个是那个红的，是 Parse-to-Parse，然后绿的是 ParseBERT。这两个... 这个 But-to-Edit 是不适用还是什么... 呃，他是没有开源代码嘛，所以复现比较困难。
A: 没有那个出... 就是那个王颖的工作是吧？
B: 啊，是的是的，对。但是复现也比较困难，是吧？就是他那个里面，他的工作都是就是隐藏了很多东西，我们就分辨不出来，所以后来我们也就没太去去... 去做他。就是没... 实在没找到就也就算了。
A: OK，就是说呃... OK，那就... 那也就是说你们现在这个做的结果呢，就是说你就如果 Parse，如果按照 Peng 的文章的 Claim 的话，应该是出现在你这个线和这个线... 他这个... 他这个 Parse-to-Parse 线之间的，是吧？
B: 啊，是的是。但是我们当然不知道它到底存在哪，对吧。
A: OK。呃，这个回... 回来我刚才讲的那个 Fine-tuning 还是不 Fine-tuning 的问题。就是我们之前做的那个... 就是 Limited Experiments，就是在 Zero-shot 的情况你下做一些那个 Few-shot，就需要给一些 Example。但是 Example 主要目的，其实不是 Train 的，不需要 Train 的，因为它的知识已经足够丰富了。我们的 Instruction 的主要目的是不是得控制这个输出模式？就是输出方案，就是 Define 嘛。嗯，因为那个... 这种 Prompting 的这种方法有最大的问题，就是它输出格格式不稳定，那么所以要用 Example 来 Regulate 输出格式。这好输出格式才能够符合我们的 PCFG，就 Semantic 上 PCFG 这种... 那种我们需要的格式，包括 Implicit 的那个 M-Path 那个 Span。因为我们现在其实已... 我们在做实验的时候，其实已经把那个呃... 多语义的结构已经整合到输出结构里面去了。因为... 因为 LLM 是能够理解多... 多多呃... 多义的 Parse，那么其实就是把之前那个... 就是阿德之前那个我们那个 PCFG 那个 Semantic 上的 PCFG 那个语义，把里边每一个 Segment，每一个 Segment 是允许多个 Segment 输出的。呃，就反正是个... 你可以把那个语义稍微改一下，这样如果允许得多个 M-Path，这样子就是那个可以多嵌入，可以每一个 SF 可以是有多个可能性。就... 哦哦哦，也可以是 SP 有哦哦... 就是其实有几种不同的语义可以表述这... 描述这件事情。那么然后的话啊，就是从当时的那个... 他的这个还原这个 Linear/Layer Transformation 的这个 Power 来看，感觉他是异常强大，不是一般的强大，是非常的强大。所以我的感觉是 Fine-tuning 一定是画蛇添足的，就是从当时的那个... 那个测试结果的那个感觉来看。那么当然你可以说，如果是完确定之后，它会更特化一些。
A：他可能会更稳定，那是有可能。就是说，因为之前因为他的性能太，就是他的工作太general了嘛，就是他什么都可以干，对吧。他当然这个也可以干很好，但是因为他干的事情很多，所以他有时候会容易发散。那么，可能可能就是你的fine-tuning可能会帮助他consolidate，就是fix啊，把这个task就变成了它。他并不是说变得更聪明，是说他的输出的话不会...就是更不那么creative，所以有时候会不会divert from the task。

A：然后呢，就是说这里有个最大的好处，就是说你你只要要做，或者是做model training，对吧，你只要做这些东西，你的问题就是你就需要做ground truth。那么ground truth要么就是用P_new，要么就是用那个你现在的这个step发现了这个所谓的pattern。Ground truth呃，它是有它是有开销的对吧，就是说因为这就是所谓的zero-shot和few-shot，还有fine-tuning的差异嘛。就是你还有就是为什么LLM跟传统的模式类的model有差异，就是，LLM毕竟需要feature extraction（特征提取），对吧，所以才会有这个label的问题。

A：所以就是说现在这个趋势，你你如果用了LLM，那肯定是尽可能的往不用fine-tuning和小样本few-shot example，还有就是简化prompt的这个这个思路去去走，才符合LLM的这个特点。否则的话就不太像LLM，那你不如干脆弄个transformer好了。

B：嗯，对。其实其实我们我们做这个号的那个想法，其实就是因为我们觉得这个规则这一块就是就像您说的，就是，呃，比如说我给他一个pair，他是可以去去去找到这个这个这个这个这个这个规则的。但是如果说比如说我是直接让他去，去帮我去去去去生成一些规则，他这个好像是比较困难。所以我们当时想的就是，呃，我能不能用那个我们这里其实也就是用那个transformer去，呃，在那个大语言，不是，那个基座上面，我们就重新做了一次这个fine-tuning。就是这个也是我们当时这个选的这个技术路线的一个原因。

B：然后呢，最终说到的，就你说的输出规则呃做不到是什么意思。就是我们的目的呢，其实是比如说我想输入一个口令，然后呢让他去帮我想一想，就是说如果我要去这个呃去改这个口令的话，他能给我提供那个，这个那几种方法。嗯，就相当于是反过来嘛。

A：这个这个不就是password model嘛。这个和你之前做的普通的，不是做这种Old-to-New的方法没有本质的区别，是这样。就是你你首先是发现了pattern，你首先在用那个...你还是有training set，对吧。你这个有training set，你Old-to-New的这个training set，通过充分训练的话，你可以建一个呃pattern model。这个pattern model就告诉你呃，whatever old and what new will be，对吧，然后what is the probability of... 你其实就是这么回... 这和那个，你普通的这个报本质上没有多大区别。

B：呃，主要是这个，呃，对，就是呃其实是差不多。就是比如我输入一个口令，然后他去输出一个口令嘛，但是那个我们之前的相当于是什么呢，是去输出的，是那个一个是口令嘛。那我现在输出的其实是规则嘛，就大概可能就是这这个主要是这个区别。因为我就就相当于是我这里做的，其实并不是一个完全的一个这个口令的一个model了，他是口令，然后再带上那个我的这个规则变化的这样的一个model。

A：Model就是model，其实都是基于pattern，不是基于pair的是这样式。Pair是原始数据，你生成的模型其实是pattern。对，那么你除非说是他... 他现在的其实包括这些PassGAN这种，我估计他其实也是这个技术路线，只不过，只不过它的性能是用概率来体现的。就是说你的目的毕竟还是说是猜测从一个old到new有多大的概率，你能够match它。

A：嗯对，呃呃在你遇到一个新的Old的时候，你的那个输出New仍然是类似的。那么对你的这个模型呢，你的模型越丰富，你的这个match的概率越高。那么传统的它... 他之前的这个文献里边的这个工作，呃，他们支不支持这种多语义的这种情况？还是说是，不是多语义的，都是每一个unique old，肯定是输出这个unique new，没有什么model？

B：以往的话它也就是这个用的是一个unique的，就是，呃，就old和new它是一一对应的嘛，就是它是有一条这个路线。其那个直接其实是有一个，它其实也是有个implicit的一个guess。对对对对对，是对，是一... 这不是不是1到多，对。

A：那我的想法就是说从更general的角度来讲，你做的一件事情其实是从，每一个old到new的这个pair。然后呢，你别忘了old到new pair其实还有这种情况，我不知道这个张浩你是不是已经注意到了，有些pair他的那个Old是一样的，但是New是不同的，你有注意到这种情况吗？

B：肯定有，嗯，对，是有的是有的。就是意味着这不是个一一映射，这是个一对多映射。

A：呃，对，这个在我的印... 呃，这个你现在是怎么处理的？

B：呃，我现在就是就是您说那种，就比如说一个用户他在比如说三个网站，三个网站上，他有三个密码嘛，那他相当于就是有一个Old的对应两个应用，你是这种意思吗？

A：呃，这是一种。还有一种就是两个不同的用户用的同一个密码，然后然后是他们在不同的网站上的这个，这个建议的新密码是不一样啊。好的，呃，我在这没有什么区别，你跟... 因为因为本来这个密码属于谁，其实不重要，就是说你看到的其实就是pair。

A：Pair当然指的是同一个人，那么只不过你这个pair，呃，你如果固定呃就是Old密码，然后，你肯定能找到很多个pair，它的第一个密码是一样的，第二个密码是不一样的。至于说这这些pair属于一个人还是多个人，not so important... maybe important。如果你要做personalize model，它是important，对吧。也就是意味着你必须要找那个personal自己，那个自己可能只是，你可能数据数据量只有5到6，这个数据量只能做撑死了，只能做那个few-shot，对吧，嗯，或者最好做zero-shot。

A：那么你现在做的呢，因为不是personal的这种model，那么那么从你的角度来讲，呃，每这这不同的pair属于不同的人，其实也有一定的也有一定的意义。你在处理的时候，如果比如说你，呃，固定password，如果说到pairing，你有10个不同的pair，呃，password都是一样的，这10个不同的pair属于三个不同的user。一个user是有5个pair，另外一个有三个pair啊，另外最后一个有两个pair，把这个10个分成三个cluster去算。
A: 可能会是一个比较命令的做法，对吧？
B: 嗯，对对。
A: 那么你现在应该其实没有做到这么复杂的这个地步，对吧？
B: 啊，是的。
A: 你现在其实是把这10个整个的就当一个人来做，对吧？
B: 对。相当于模型就反映那个……就是哪是一个谁，是哪种。个数越大，他相当于就越倾向于反映他的一个情况。
A: 那也就是说，如果说这10个里边有5个都是User A的话，你其实觉的是User A，嗯，User B and C，对吧？
B: 对对对。所以这个就是你的……就这个其实造出了很多冤枉，对吧。
A: 嗯，呃，然后就说但总体来讲，就是说其实你看这里面的实际问题呢是：因为你有O to New（Old to New），那么如果说传统文献里边都是O to New，不管是呃……但是他也不可能只做一个有那个One-to-One版，因为他这里存在多个映射，他怎么可能做完One-to-One？除非说他跟你做的一样，还是走从多个Pass里边选那个Most Pass。所有的人都是做了这件事情，就是说你做的这件事情，别人做的也是这件事情，是吧？只不用了不同方法而已。
B: 呃，对的对的，就是他们也是就是类似的。就是比如说像那个，呃，像您提到的，比如说呃我一个人他可能在三个网站上，或者说我这个不同的人用了一个同样的口令，然后在不同的网站上，他有不同的口令，他都是把它单独的抽出来一个这样的一个Pair的。就是比如说，呃，就像那个就刚才张连浩讲，就是那个我一个人，然后在三个网站上有，那我就会抽出这个两个Pair这样子。
A: 对，我知道输出两个Pair，这个没问题。输两个Pair的问题是，两个Pair的如果转成规格一样，你选哪一个？如果是两个，你连那个都有少，题都没有，都有都都……都会在的，都会在的。都会在那就不是One-to-One了，对吧？
B: 对，对对。因为他是这样，就是因为他毕竟History是不一样的了嘛。所以说他肯定就是不能说是是这个我们要选择，所以说他其实的话是也就是说在你有不一样的情况里下，就说他有……他有每一个他的那个模型啊，就是Path Model。嗯，就说在固定配的情况下他会有一个Path，然后不同的Old，Source Old不同的，你有的是有不同的那个……不同的那个Probability Model，对吧？
A: 也就是说如果是Old有10个不同，10个Branch都在，1个Channel都有不同的Probability，大概这样？
B: 对对对，对对对对。
A: 那这个这个没什么，你其实现在做的在这点上和他们……就是我在想，其实在Set up上面其实没有区别的。
B: 对。
A: 主要的区别其实就是你的模型不一样。
B: 对，方法对上是不一样。
A: 等于是别人没有用……别人没有用LM这个层次的Transformer，对吧？
B: 对对。
A: 然后因为你用了车（Transformer），他们也没有做真正的Training，他们实际上是一些Rule based的方法，是吧？
B: 嗯，他们的这个Training其实也只能要……如果是有的话，也只能就是说，呃，怎么说呢，就是比较比较简单的那种这个神经网络的这种，就是比如说RNN嘛。就是他那个王定的那个的Paper是也是用的LM，印象中应该也是的。
A: 就是说呃，其实就等于说OK，就是这么讲，就是说实际上也就说这个文章呢其实主要的Novelty或者是这个Difference is to use a much larger Transformer, which effectively speaking is a smaller LM base model，对吧？
B: 对对。
A: As Transformer然后做，Classifier之后，你再用那个Classifier build a model，然后去做呃Generation，对吧？
B: 呃，后面的话因为我们已经生成出规则了嘛。那这样的话我们就能去通过那个……那个就是Code，然后加规则，我们就会生成这个。这就是这个就是了，所谓的Path Model就是一个……就是其实就是Password加上这个不同的Pathways and Probabilities。
A: 对对对。
B: 就是我……我们这个Model，我们这个Model它的目标其实就是生成规则，就是把这些规则对应的这些这个针对性的规则生成出来，就这个是我们的目标。
A: OK，然后的话我再问一下，我现在很理解怎么做的事情。然后你这个Path Model的这个Input Password用的就是来源数据集里边所有的Pass，没有再做整合是吧？
B: 呃，对对。就说比如说你刚才如果是Source里边有200万，那你这个Path Model其实就是200万，加上其实就是200万的Set of passes and probabilities就是你的Model，对吧？
A: 嗯，对对对。你这里叫Rule Class Probability。就说如果说平均每一个密码有两个Pass、两个Probability的话，你这个200万就会有400……400万个这个呃Pair，就这样对吧？
B: 呃，对对对，是。
A: 嗯，然后的话实际上这个跟文献里边的做法也是一样的，是吧？他们也是用的这种几百万的这种Connection或者这种Prob Model。只不过用不同模型去训练，或者用Rule来去呃……或者是不不做训练啊，但是总是有一定学习过的。嗯，还是说他们的模型更小？
B: 呃，就是对于Pass-to-Pass、Pass-to-Edit的话，他们也是采用这种方式。然后那个Passport的话它是自己有一个规则集，就是它是只针对那个规则集去啊做那个。就是比如说他那规则集有5万个、5万种规则，他就是把那5万种规则做了一个Probability。
A: 他这个5万种规则指的是把Password的那个结构记录下来是吧？不是记录个400万Password、200万Password？
B: 呃，不是不是不是。他他就相当于就是统计了呃所有可能出现的规则，然后把那个做成一个规则集。
A: 规则是转换规则，还是说是就是转换……就是比如说那个Hashcat那种转换规则？但是转换规则不适合Password的，不是Password dependent。他这样，如果没有Password，他怎么知道这里是有哪个规则？
B: 呃，我印象中啊就是那个Passport，他好像是那个用的是就是他……它本身它也提供了很就是好好多个的这个这个Rule嘛。我印象中他好像用的是其中的一个比较大的那个那个Rule，然后它里面应该是有5万多个Entries。应该是这个这个概念，就是他不管多少个Entry，他这个Password呃Dependency有没有……如果没有的话，那他就是一个……他是这样，就是Hashcat他自己提供了一个工具，那个具体我没看，就是他是可以，比如说你输入给他一个这个口令集……
A: 然后呢，他去根据这个口令集，去生成一些对应的规则。就相当于是说，我们最开始的那个A*算法是一个口令集还是两个口令集……我一开始是一个口令集的话，那就是我要拿去算 PCFG 的动作。如果是第二个口令集，才是他当时做的……好像不对，是他当时跟那个 PCFG 比赛机制还不太一样。因为具体细节我没太细究，他有可能是，比如说我输入一个口令之后，然后他去做了一个排序，把相近的那些口令拿出来。拿出来之后，他可能用的就是类似于我们A*的这个算法，算了一个比如说从这个口令转换到另外一个口令大概需要变换成什么样的一个规则。然后他把这些都记录下来，做个排序然后输出，大概应该是这样的一个流程。

B: 对，但是你这样等于输入还是两个密码……呃，两个口令集？不是一个后的……

A: 就一个口令库也可以做到呀。就是一个口令库，他就直接对这个口令库进行排序了嘛。排序了之后，然后他去选择比较相近的这些密码。

B: 就比如 password 1 和 password 12，同一个口令的 similar password，那属于是不同 users 的这个……

A: 对对对，对对。

B: 他是觉得不可……但这完全就不是一个问题，理论上都不是一个问题。因为预测同一个人怎么去改密码，和不同的人之间怎么样 accidentally（偶然相似），我觉得这是完全两个不同的问题，不应该混对同一个。

A: 但是就是在他们实际用的时候，因为 Hashcat 本身是个工具嘛，实际用的时候，他其实就是想：我针对一个口令，我不管你是这个人用的，还是多个人用的，你能不能帮我去生成出更多的……就是说他的 password 复用，不同用户同一个密码的方法建构起来的。

B: 但他在做攻击的时候，也把它应用于了这个同一个用户的不同的库之间密码变换这个场景下？

A: 哦，这意思不是的，不是的。PassGAN 呢它是这样，它是拿到的这个规则，这个规则本身印象中应该就是 Hashcat 的那个工具生成出来的。相当于是说是一个群体的复用，就我们群体复用的那个概念。就是比如说不同用户他们用的比较相近的，他也认为这是一个变换嘛，然后他就拿到了一些这个规则。然后他以这个规则，把它输入到了他那个 BERT 的那个模型里面去。然后呢，他去生成出了一些新的规则，做了个分类。当他实际去 cracking 的时候，他也是拿到了一个原始的口令集，这个口令集也是不区分是否有相同用户的，也是一个混杂的口令集。然后应用这些 extracted 出来的规则，去攻另外一个库。大概应该是这个流程。

B: 对，就是这是我讲那个意思嘛。就说他用 PA，只不过他的 PA 可能不是 Old to New 而是 Two……

A: 嗯，对对对。然后它生成呢和 Input Password 应该是无关的规则，对吧？

B: 对，否则的话他那个 Model 的体量就要上百万或者是千万级。对对对，因为他没有记录，他只是记录了从任何一个 password 到另外一个 password，它会有哪种可能变化。他是出了几万、几十万这个量级的变换规则、概率图。

A: 对。

B: 然后有了这个概率图之后，他可以在同一个数据集里面从一个 subset 预测其他的，也可以从一个 Password Database 预测另外一个 Password Database，同一个 User 的或者不同 User 的。当然，他也不 care 嘛，因为他只是只针对这个。但是我个人觉得原则上来讲，这样的话其实是把两种不同的情况给 smooth 掉。

A: 呃，对对对，是的。因为那个不同的用户的相似的 sharing，就是我们刚讲的所谓群体 sharing 和个人的 sharing 应该还是不太一样。哪怕是个人的 sharing 在群体上的一个整合，可能也是跟那个不太一样，只不过群体这就话可能跟那个的相似度可能会更高一点。但如果是要是转放在 personal level 那肯定不一样。然你们现在当然也没做 user level，所以可能稍微还可以 argue 有一定的可比性。

B: 对。

A: 其实就是我们这工作啊，那个一开始张林浩刚进我们实验室的时候，我们当时想的其实就是我们的这个目标，其实是想把这两种复用去区分开的。就是针对同样的用户的这个复用，以及针对这种混用的复用。我觉得如果把它分开讨论，可能会更有意义。因为至少目前来看的话，目前的文献当中是缺乏对这两个复用的讨论的。就是同一个人对自己密码的变换，和很多个人混在一起密码相似度的差别，其实是没有人讨论的。我觉得这个其实也是一个……如果说比如后面我们可以把您做的那个密码可视化那个工作如果能引入进来，我觉得这个应该也是一个比较好的思路或者扩展，这其实我觉得是具有一定 Novelty 的。

B: 这个密码可视化其实跟这个差异比较大，因为密码可视化主要目的不是做 cracking，它主要目的是为了做分析。你这里的话，我现在就是想，这里的主要问题就是说要看 Depth and Novelty 在什么地方。就是它到底解决的是一个什么问题，另外就是有没有进一步提高的空间，尤其是简化它的架构。因为当然了，如果说你在不简化架构的情况底下，你已经能够比文献的结果更好——就像你现在这个蓝线显示的，实际上比别人好很多——但现在最后这张图是个啥意思？

A: 对，这个最后一张，就是在我的那个……因为我是分不同的规则复杂度去做的验证，相当于最后一张它是在有两步需要经过两步变换才能够生成的验证集里面去做的。然后在这里面的话，就是我们的这个结果和那个 Pass2Pass 的结果是相差不大。

B: 这个“两步”，其实就是指的两个 Levenshtein Transform，是吧？

A: 对，就是两步之上吧。

B: OK，就说你的实际上的主要的 improvement，都集中在最简单的 Levenshtein Transform 是什么？

A: 是的。

B: 就感觉稍微有点反直觉。因为你用的是一个很复杂的模型，但是其实他能干的事情，基本上只能干一步的 Transform。这个可能是这个事情其他的模型好像还做不了，可能稍微有点反直觉的感觉。

A: 对，因为我的那个……嗯，啊。
A: 喂？嗯，可以听到。哎，好奇怪啊，我不不知道为什么拉不了你。
B: 哦，哎，那可以直接把李教授拉进来吗？
A: 我现在正在拉李教授啊。好，OK。呃，对，呃那个其实刚才讲，就是我的想法是这样的啊。就是我们稍微就是回到一下，总结一下。就是说，目前的这个结果，如果说是只看纯数字，呃，在一步变换——实际上就是一步变换——如果确实比别人做得更好，这个工作就说……呃，当然你用的是LM对吧，你可以说是因为用了LM这样一个更大的Transformer的一个模型，所以比那些RNN啊whatsoever别人用的东西强。这是你的区别。你的区别就是，别人没用这个，别人没用LM这种做机体来做fine-tuning，你用了LM来做，所以你在单部的转换上比别人做得好了很多。
A: 呃，当然如何处理这个fact，我们要思考一下。就是假设说，考虑任何变化，呃，只用这样的方法，只用这样的结果，那么你现在的gap就是LM as a classifier，than classifier which significantly improve the one-step change。然后这个结果听起来是非常surprise，就是因为one-step应该是很容易的，对吧？
B: 嗯，对。
A: 那个为什么其他的模型做得这么差，可能要有点discussion。然后就说就是如果只是说目前的工作看怎么样去发表的话，那么这里边的重点可能要在discussion上面可能要做一些工作，然后可能甚至要……甚至要做一些分析。比如说看一下，之前的那些工作到底漏掉了哪些？为什么他们没有抓到那些很明显的、很简单的这个one-step trans？是他的入库的规则太大还是太小？还是说是有些变化就是我现在其实都没有办法去imagine。就说如果是一步变化——但我没有看那个Path-to-Pass这种规则变换——如果是一步变化，他应该是全都可以看过了。但他也可能复制概率不一样，但复制概率有什么会……会怎么会不一样呢？你的概率也不应该比他的有什么优势啊。就说，很难很难在定性的角度上去理解，就是你用了一个大的模型之后，怎么样就能够更好的去覆盖这个one-step这个trans方……呃就是这个rules。就感觉没有一个特别intuitive的explanation。所以这方面可能要做点工作，要不然的话，Reviewer会觉得你这个结果是造假，要不然这个就是不合理，就是完全是counter-intuitive。
B: 嗯嗯，好。
A: 呃，所以就是这是就目前的结果来讲，那么那么就是说你的novelty就是……larger than others specifically……你加了ground truth。虽然我个人对ground truth是有一些意见的，但如果别人其实也没有考虑这个multi-pass的问题，就是同一个pair multi-pass的问题，不是说是之前讲那个m-pass的问题。那么别人没讲我们也不讲，我们也不用去管它。只不过就是说如果将来要真的要做用LM来做zero-shot，我觉得one-step是一定要加进来的。加进来之后，这就是一个跟别人的完全不同的differentiator。就是说因为之前他们是没有考虑过同样的pair还是有multi-pass。
B: 嗯嗯。
A: 这个mode只会出现在超过了两次iteration之后的这那个才会出现，对吧？
B: 对。呃，呃这个one-step是不可能的。有……不对吧？因为它是……它是……
A: 那么这样的话这就变成了，如果是做那样的事情，目的就是为提升你的那个两步、三步，就是多步的变换那个性能。因为你现在的这个东西是没有多步多步的这个性能提升的嘛，对吧？
B: 嗯，是的。
A: 嗯，那么那么现在的问题就是说，如果是现在的这个结果，我觉得可能主要问题就是要去解释你的结果为什么能够做别人做不了的事情，然后肯定要有一些discussion。如果有些discussion的话，那么……你反正有实验结果。呃，除了这个Path-to-Edit这种，呃，所以的这个东西如果是硬和他的文章类的结构比较，你能够画出他那条线吗？还是说你没有办法实现复现他的实验？
B: 呃，对，因为我们基本采用的验证集都不一样嘛，所以他这个……所以就是没办法用他的文章里的数据。对对对，也没有代码，也没有文章数据可以用，所以说我们其实是不知道它的线在哪儿的，它的线还是有可能高于我们的线。
A: 如果你一定要就马上给……嗯，是，对，这个地方就是就是比较有讨厌的。就说，那这也是为什么这个领域——其实我自己也personally dislike这个field这一领域——有一个原因就是不太也不太容易去做更多的工作。就是因为里边有大量的工作都是王宁他们做的，然后但是大量的工作呢，他们都是不放代码的，所以就是在那个这个领域如果要做比较，arguments会更难一些。其他的领域这样来讲啊，不是王宁他们over-dominate，所以就说哪怕把他们的东西丢了也也不会影响我们的一些comparison。所以从发文章的角度来讲，就是这里边有两个barrier：一个barrier就是怎么去解释这个……怎么去解释呢，你的这个performance是哪里来的；另外一个就是怎么处理这个comparison。
A: 嗯嗯。但是如果说你的做法如果增加了这个multi-pass的话，而且不用……如果是不用training……呃，但就其实他用的也是需要做一定的learning，但是没有training。就是如果是做LM的话，呃，LM其实是其实是没有training，连learning都没有。说实话，你要是一定要说要learning，其实他那个learning就是reconstruct fact model。但是这个reconstruct fact model总是要的嘛，因为你一张白纸，你得从得把这个模型建出来，对吧？有了数据它可以建模型，但这个不中心是不涉及这个train。那这个哪怕不能跟Path-to-Edit去比较，至少方法学上有substantial difference。那么就可以绕开了这个Path-to-Pass这个比较直接比较的问题，因为毕竟方法这个差异很大。他有没有考虑这两个关键的问……关键的问题？因为它也是有training点的对吧？
B: 嗯，是的。
A: 那么可能范围上就会更容易一些，而而确实他的那个value也会更多一些。那么当然如果说你在这个多步的……这个多步的上面，如果能够超越目前的这个红色的那个线，那是Path-to-Pass是吧？
B: 啊，是的是的。
A: 如果是在多步的这个上面，如果能超越Path-to-Pass……
A: 那就显得就是说，因为一旦是多步的话，事情就复杂了，对吧？然后你说你用的更复杂的东西，能不能处理得多步处理得更好？这里边业务好像 make sense，对吧？嗯嗯，OK，反正大概就是这样子。

A: 然后如果是现在的这个工作，如果要是想试着发表的话，我估计上顶会可能会比较困难。就是说主要的原因就是，就刚才讲那两个原因：一个是结果的比较问题；另外就是方法学上的话，其实应该说主要的变化就是用一个更大的一个……我是用的 model 应该是 Large Language Model，nothing particularly different，对吧？
B: 呃，是。

A: 然后的话你的结果证明的话，用了这个 Large Model 之后，你处理的主要是 one-step rule。这个的话我是觉得是肯定要解释的。就是说，解释的方法其实也很简单，你无非就是把你的那个 rule set 和他们的 rule set，然后做一下差集，然后看一下你到底比他、相比他们来讲增加了一些什么。然后分析一下为什么这些差集别人没有抓到。

A: 然后如果看到这些差集之后，你能够发现，哦，这就是为什么。因为其实我现在还是没法 imagine 为什么，因为一步变化很简单，一部分连语义都没有，你就做 Edit Distance 嘛，对吧？
B: 嗯，对。

A: 如果你没有什么 smart 的算法，你只做一步变换，就做 Edit Distance。然后源头来讲，最简单的 Edit Distance 这方法应该比你的 LLM 的方法都好，不是吗？
B: 嗯，是。

A: 就是 Edit Distance 这个方法有人做过吗？就这是最简单的，就是 PassGAN 的思路吗？不是 PassGAN，别人用的是什么？算也 RNN 是吗？
B: 呃，对，也是 RNN。

A: OK，OK。反正你如果知道我现在的这个疑惑的话，reviewer 就会有同样的疑惑。因为你毕竟是单步变换，单步变换就太简单了。然后单步变换以前的话，我可以说他们多步变换也能够处理得很好，所以说你们现在做的也没什么差异。然后，但是单步依然还是做得很差，多步还怎么可以做得很好？我现在也不是很理解这件事情，因为连基本都没做好，怎么可以跑起来，对吧？

A: 当然 Edit Distance 算法只能算差异，他其实可能算不了那个题……也能算。Edit Distance 的算法其实好像是可以算出那个……哦，对哈，好像还不太一样。有个缺点，但是他那个缺点跟你现在这个缺点是一样的，就是说他是考虑那个……他其实就是 A* 那个味道，因为 Edit Distance 其实就是寻找那个最小的 edits，对吧？这 number of operations 其实就是你的 HashCat 的思路。所以如果说在不考虑多余 path 的情况下，好像 Edit Distance 和你的 HashCat 其实同一件事情，是吧？你的 ground truth 其实就是 Edit Distance 算出来那个，没什么 way，对吧？
B: 嗯，对对对，就是他的优化目标就是那个，然后的话就是用 LLM 去学这件事情而已。

A: 啊，是的是。但是如果是单步的话有个最大的问题，其实不需要学习的对吧？但是单步就是，你直接覆盖率就完了嘛，对吧？因为他这没什么好学的，就是一个单步变化而已，加减或者是删嘛，对吧？就这三种类型。
B: 呃，加减或者删或者是 modify，对吧？
A: 嗯，对。

B: 但是在我这边，如果说用 HashCat 的规则去表示的话，它的种类就很多了。就比如说就单替换这一个操作，它可能就有……就是 Sigma 的平方倍那种感觉吧。他那个是因为他没有……他不是 password dependent，所以才是这样的；password dependent 就不是这样的。

A: 是的是的。所以你现在问题是，你当然不能用 HashCat 来做你的 baseline 了。HashCat 是和那个 PassGAN 做，反而是个一类的一类的问题。就是他们是个小的 small password independent transformation rule sets。你的是 password dependent rule，两个是天壤之别。
A: 你那个图里边那个蓝线是 PassGAN 是吗？
B: 啊，蓝线是我自己的结果。
A: 啊，不是不是哪线？另外还有一条线是绿线啊？
B: 绿线是 PassGAN。

A: 对，绿线是 PassGAN。其实我觉得 PassGAN 其实是这个比较可能不是非常 fair。其实你那个多步的那个变化，绿线也是在比较低的，对吧？
B: 嗯，对对对。

A: 我个人觉得就有点类似你自己在跑，你跟别人在走路的人去比较，这不是很不是很合理，我觉得。因为模型差异太大了，因为等于是人家是零级玩家，你是一级玩家，对吧？你拿个一级玩家跟人家零级玩家去比，这不太好吧？
B: 对，赛哪了……嗯，这个是这样的。就是那个当时想去和那个 PassGAN 比，因为也是说他当时那个也是就是对这个规则这块进行了一定的学习嘛，所以说我们当时也想看看他的效果到底怎么样，所以说就也做了这样的一个一个实验，只是拿来做一下就可以。

A: 就是他们之前的那个工作也用 PassGAN 做基底做比较了吗？
B: 呃，是……呃是什么意思？没看明白。那应该是没有的，他们针对规则这块应该是没有的。因为他们当时 PassGAN 主要是说是还是针对口令的，就是那个口令这块。然后它有一些 template，然后他根据 template 去生成一些这个新的口令，它主要是这个。就说 PassGAN 主要的主攻目标还是同一个数据库，就是一个就是传统的这个 setting，不是这个。

A: 啊，对对对对对，是的，这是合理的。也就是说，呃，你们这个 PassGAN 加的可能是有一点点“画蛇添足”。那么你当然加了就加了，你自己知道就行了，你在写文章的时候可能不是非常容易 argue。那么但是如果说他变得可有可无的话，那你的这个结果的话就等于就只有 PassGAN（Note: implied previous comparison），那这个就有点尴尬了，对吧？
B: 嗯，更尴尬。

A: 然后的问题就是像（implied bad paper）这种问题的做法，就是虽然他没有代码，然后他文章写得不好，但是从大家的角度来讲呢，你又不能就是指着别人鼻子这么说，对吧？然后你也不能说你不能复现。如果你说这篇文章我们图的是复现不了，好像在文章里没法这么写，所以就很尴尬。这个地方，这个这个实在不行，就就这么说呗，因为人家这么说，肯定发不了顶会。
B: 对，我们这一块反正这个到时候我们也也也要看一下，就是这可能也确实是这样。我这个反正这个确实还挺难的，我觉得，这就不好弄。
A: 对，所以我的想法、我的想法是这样，你要不就是想分步走，如果现在的这个工作……
A: 你觉得能够把这个一步的原因，至少定性的、定量是因为如果你做了那个Difference Set的话，其实是可以有一些定量指标，肯定会有一些定性的解释。如果能解释清楚这件事情，哪怕比PassPass结果好，那么Pass2Edit算法不能实现……啊，我在想有没有什么办法绕。他的问题主要是因为他的算法有点复杂，我当时粗略看了一下，感觉也是那个老问题，就是说他的算法其实是有点复杂，但是讲得很难、很不清楚，是吧？
B: 对，是的。所以就是说哪怕部分实现都很困难，比如说只是想把他的Rule抓出来，那都比较困难。
A: 对，就主要是那个只把Rule抓出来就意义不大嘛。因为不是只把Rule抓出来，我们可以做的一件事情，就是在做那个一步规则变换的那个情况底下的时候，是可以做那个差集差距比较的。
B: 嗯，这个到时可以去看一下，因为这个也没特别细的去看，我觉得后面我们都可以去学习。然后另外就是虽然汪迪从来不是Share代码……
A: 但是这个你也没有联系的，当然也知道他们不是那么友好，所以联系过去也不会有用，但反正你没联系过是吧？
B: 呃，没有没有，没联系过的。我们之前联系过的就是那个其他的代码，他当时就是也没给嘛。
A: 对，我知道他基本上是他们的一贯的作风。
B: 对，但是那个他们2025年的那个PaLM，他们当时Share出来了。出来是不是因为那个Reviewer要求？
A: 不好说，有可能因为那个顶会现在不是都要求这个，对。因为现在一般都在里面，他如果有那个Open Science的那个Section，所以就是说如果他没有合理理由，他Open Science那个Section没法写。所以有可能会变成的是从现在开始，有可能他们不得不放代码出来。
B: 对。其实我们看了他那个PaLM的代码，那代码也很有意思。就是之前那个张林浩同学，他去用那个Model去跑了一下。他那个很有意思的是什么呢？就是他们实际上那个文章里用的都是那个蒙特卡罗嘛，但是实际上我们也想看看他实际生成的效果怎么样。然后实际生成的效果呢，就是当时张林浩同学是设定了一个概率阈值，然后去生成出这一批量的一些口令。很有意思的点是，比如说我输出了这个200万条口令，然后呢我再去输入一个稍微小一点的概率值，让它去生成更多口令的时候，它实际上生成更多口令的效果，还不如我就生成200万的那个效果好。就他这个很奇怪，他用那个蒙特卡罗测试的方法感觉上是有问题的，就很难评估他在实际生成的时候，他的性能到底是怎么样的。
A: OK，但不管怎么说他代码是可以跑起来的对吧？
B: 对对对，跑是可以跑起来。
A: 能跑起来，你就可以重新再做实际真实密码的攻击结果嘛，你就可以跟蒙特卡罗做比较，不就是这样吗？就是他的比较就是按照他那个实际生成的代码和蒙特卡罗比较非常大，对吧？所以说这个工作我们如果说，原则来讲，评审应该是这样的：就是如果说只写一篇文章，这篇文章就是有点类似我们那个SoK的，只不过是可能是有点个案分析的。你就把这篇文章拿来，然后把他的代码跑一遍，用真实的数据做一下，然后再和蒙特卡罗比较一下。然后这就是一个 Real counter example of why Monte Carlo should not be used and should never be used in future password studies，对吧？
B: 对，我们这块……是不是可以照着这思路去思考一下？就是说当然最好是不是只有一个个案，如果有两个个案，那就可以稍微分散一下打击面。这样的话这个工作就有点类似一个mini SoK，但是我们不会叫它SoK了。对，就是说 Why Monte Carlo should not be used in……对吧？
A: 嗯嗯，然后文章很简单，就是如果能找到两个例子，就是 Use two counter examples to show why the difference can be so huge that they can completely reverse the results，对吧？
B: 嗯，对。我们发现这个事情，实际上现在有另外一个同学叫李佩莹，然后她现在就安排她做的就是这块的一个研究。就是我们其实也想通过这个事件，我们也发现了，就是实际上以往的这些文章啊，他们的那个Model、Password Model是一个方面，实际上他在评价这个模型的时候，实际上是有非常大的这种差别的。有些人用的就是实际生成，然后有些用的就是蒙特卡罗，但是实际上那个用的蒙特卡罗的方法呢可能还不太一样，去做实际生成的时候的方法也不太一样。所以说这块的话，就是跟李佩莹同学说的，让她去了解一下现在一些比较核心的或者常被用来进行比较的这些模型，就是他们在实际生成的时候到底都是用的什么样的方法，以及他们这个文章当中用的是什么样的方法。然后后面的话，我们也想看看能不能有一个类似于口令生成框架一样的东西，能够去尽量的提供同样的一个方法，然后大家以后都用这个方法去评估这个口令模型，然后去算它的性能。这个也是我们另外一个同学，他现在主要在研究的这样的一个工作。
A: 这就是SoK的那个一个迷你版，就是说SoK里边的这个Password那一部分的一个标准化。
B: 对对对。
A: 如果因为这个SoK这个东西，因为一直是本身体量有点大，Scope也比较宽，我们在时间上也一直没有保证，所以一直也没有做。所以是不是按照目前这样的一个思路去做，反而就打散个来做，反而会更容易一些，代表的内容更少。实际上之前那个SoK我觉得可能是Overkill……有一点对。如果是我们现在就集中于这个Real Password Based Evaluation vs Monte Carlo Based Evaluation，然后做一下文献爬梳，然后把这些文章能够有代码、能够复现、能用两种方式重新复现的，我们把它都复现一遍。然后我们可以说，我们不是……
A: Better. 嗯，我们就是比如说都用我们能把的做一下，把他们排着做一下那个性能比较吧。都用这个真实的，然后甚至真实有不同的setting，对吧？打到一个有不同setting。
B: 对对对。
A: 用几种不同setup，我们把结果做一下。做完之后，我们要证明的一件事情，就是说，呃，the ranking are difference。
B: Oh，对对对。
A: 然后呢我们可以说，然后the gap of performance呃 very big。就一个是ranking is different，也就意味着之前的工作里边说他perform better，it's not reliable。那么他有perform claim，也就意味着他的那个absolute value，甚至是那个level of performance，甚至it's not reliable，对吧？
B: 嗯。
A: 然后我们再去讨论，because it is simulation based，虽然他有之前的那个一定的理论推导，说他有一定的那个metric，对吧？但是their real world test indicate that it's not as good as what theory predicts. It means the theoretical analysis need to be revisited，对吧？
B: 对。
A: 然后呢在before that is settled，and for formally verifying，the real cracking performance should be used constantly，对吧？对，我觉得这个case应非常强的。那么而且基本上就有点那种，其实不需要有什么new novel，就是no，就是一个comparative study。对，showing how inconsistent results are and how harmful this inconsistent performance evaluation... 看看病，对吧？
B: 嗯，对。
A: 然后就这样一篇文章，我觉得应该，那可以上。因为这个我之前，你记记得之前我给你发的那个（论文）...
B: 对对对，我记得，我记得就是跟那篇文章一模一样，没去。
A: 对对对，是的是的。他那篇文章做的当然就是所谓的那个Time-to-crack analysis，然后七七八八的，但是也是inconsistent，只不过是那种不同的inconsistent，对吧？他当时显示的也是如果你用那种方法，呃，有一篇NDSS的文章就可以没有任何实质性的提成了（提升了），那么是应该不应该被录用的，对吧？
B: 嗯，是的。就是所以说我觉得就是这个它针对性比较强，而且可操作性其实也比较高。
A: 然后那个就排... 我的想法就是OK，那我们就按照这个思路来推动一下这个SoK的一个第一步。然后的话，因为如果要写SoK的话，要想SoK好些，那就是这块卡进去之后，剩下的可以更多的其实就是一个review，然后加上一些... 对吧？
B: 嗯，对。
A: 然后refer to那篇文章说for performance, we have another paper know which substantially showed real time cracking... Proposal书里面有。
B: 嗯，OK那这个清楚了。
A: 然后就是说现在的这个张立浩这个工作的话，我的想法就是说因为那个... 啊对，因为我觉得我们现在其实所有的工作里边，紧迫性最高的其实是那个LLM的工作。对LLM的工作的话，怎么讲呢，已经有人在做了，但是这个很奇怪，这个王景他们做的是这么奇怪的做法。那么他现在的这个power就是呃针对我们这个PCFG的这个... 我们之前讲过那个几个不同的extension。那个对我们IC下PCFG这个提速者来讲是非常自然的，就是水到渠成。而且基本其实都测试过了，就是前面的部分其实也基本测试过了。只不过前面部分呢，因为那个之前那个法国的实习生太不给力了。所以他实际上最终做的东西就是他只是手工的做了100多个样本，做了个测试，证明了多义语义是可以做的。那么而且成功率就是非常高的，Accuracy实际上比我们的线下PCFG的那个正确率好像可能还要高一些。就是这这个可是不得了，因为你之前那个PCFG原来来讲，用的都是的那种... 这种方法原来讲应该是很高的，但是毕竟还是可能作为异质性对他有干扰或者what，对吧？造成他的性能提分也就在百分之八九十的、90左右的那里对吧？但因不同的咱们这个端口的那个结果也不太一样。
A: 然后最主要的那个是，那个是This... 这个因为我们做为few-shot真的是可以，因为那个大模型的这个power感觉是，就是语义太丰富了。然后就是后边的这个C这部分我们现在已经做的，就等于说是可以把已有的任何password model用LLM来做refinement，对吧？嗯，然后也是可以直接就产生效果的，也就是不需要做前端。因为前端的那个，就咱们那个segmentation原来来讲只适用于我们来做PCFG，因为其他的人本来就不care那个咱们这个嘛，或者咱们那个非常弱，所以说没办法切进去。但是呢后端这个improvement是所有人原盘都可以做一定程度的改进。
B: 对，因为我的想法是，如果是张一浩现在做的实际上已经开始涉及到一些LLM的这个一些工作，虽然现在做的方法应该说其实是没有充分利用LLM的power，其实是把LLM当一个Transformer来用嘛。
A: 对，嗯感觉有点说实话呀就是杀鸡用牛刀对吧。
B: 对，这个其实是... 也不完全是，因为那个我们最开始其实也是通过那个王静的那篇的那个论文，我没想到不同的这个路线，他对于这个规则，他这个到底效果能怎么样，这个我们也也是有这样的一个想法的。
A: OK呃，现在最主要问题就是说你和那个Toto (SOTA?) 这个方法学上的本身差别，他们用了LLM对吧？不只是用了RNN，好像也没有用到MO... 他最新的那个Retraining也是做some password吧？
B: 不是呃，那个不是，那个是用来做那个... 就是正常... 那个他是这样的...
A: 他说了，他说那个这个来做了对对对，都做了，对吧？
B: 对对对。
A: 他就是里面那些小的章节……他没有去做的Person那块是怎么做的呢？
B: 他是这样的，他是把那个个人信息都编码到了他的那个输入里面去。就是他是分那个Person Information，然后分为一类、二类。也就是说，他把这个数据建立起来，相当于是整个的Person Information和他的这个口令一起做了一个编码，然后放到了那个Transformer里去进行了一个Fine-tuning。
A: 也就是说他其实是用了Additional Information。
B: 对对。
A: 那张浩现在做的工作呢是没有这个Information，那没有的是用了Password Itself，对吧？
B: 对对对，是的。
A: 那么也就是说，LM（Language Model）本身的应用应该说是比它的要纯粹一点，而且应用范围会更广一点。也就是说只要有个Email或者有ID，你就可以用，不一定非要有其他特定的东西。
B: 对，然而是一个更Generalized、更Universal的一个环境，不是一个非常Narrow的。
A: 哎，对对。OK，反正我的想法就是，如果你们要是想把目前的这个工作告一段落……我们要思考一下怎么样去做。PA（Personal Attack）做不了这个，上顶会就要看怎么去讲，肯定要一段话去解释这件事情。但是你如果只有一个Benchmark，感觉还是有点太Small，除非你说“This is the best, so we use the best”，但这很难说。
B: 嗯，我的想法可能是，就是我们这篇有可能就先不去冲顶会。就是我们可能会发一个稍微弱一些的，哎，对，C类、B类类似于这种会。
A: 对对对。
B: 我们先去试试水。因为通过这个事情，我们发现其实后面能做的东西还是比较多的。就是后面我们把它集合起来之后，比如说像那个CPCFG的那个……到时候张浩也可以接这个接口来继续做嘛。因为我现在在做的是针对那个之前没分出来的那些的一个改进，现在就是看起来还可以，也就是说后续我们还能去做更多的东西。所以说这篇文章的话，就是目前已经做到这个样子的话，我觉得算是一个阶段性成果，我们想看看能不能先冲个B会先去试试水。后面的话，因为张浩现在还是大四，他后面包括研究生阶段，也可以继续去研究这一块的内容。
A: OK，那我觉得也是这个思路。我的想法就是说把目前这个工作稍微整理一下，发一个C/B会。然后我的建议是，能不能让张浩转到用LM来做分析？当然如果最好是先把CPCFG这块做了，然后我们再去做这个Personalize。因为我总是觉得Personalize这个部分，说实话、说真心话，我觉得完全Useless。
B: 为什么呢？
A: 因为其实事实是，现在已经是什么时代了？你还做这个？现在这个时代真正的一个正常人都在用Password Manager（密码管理器）。确实，你会重用密码吗？你不会的，你真正会使用密码的地方，根本用不上这种东西。所以感觉这真的是为做文章而做文章，有意义吗？我是很不喜欢这种做法。而那个普通的密码攻击可就不一样了，对吧？这个就是因为Password every day，每天都有。你如果是能够用已经有的例子的Database，能够把一个新的刚出来的Database尽可能的全部还原出来，那这个是非常Quick、非常有用的。反正他用的这种方法，应用渠道越来越少了。
B: 确实。因为本来密码现在的重性就已经在下降了，因为MFA（多因素认证）的问题，对吧？
A: 对对对，就是换密码又咋样？换密码没什么用，说实话，真的。
B: 确实确实。但是如果你把一个Leaked Database整体的给还原出来之后，那里边的间接影响很大。它不是只说你可以登录（因为这里有MFA可能登录不了），但是你从里面可以抓出很多Personal Information，抓住他的那个Behavior（行为），你可以做Phishing（钓鱼），对吧？
A: 其实我觉得在现在这个时代，已经不能再去用老的那个思路去考虑什么Cracking Password，然后Crack完你就Success了。Not really，because you didn't achieve anything because to authenticate means password...
B: 其实现在就是个人口令确实像您说的，就是登录很困难。但是其实有很多个人信息，无论是在本地也好，还是在云端也好，它其实都是用那个口令做主密钥，然后他去生成一些派生密钥，然后再去保存的。这些信息其实反而可能会更重要。这些信息的话，真的需要对密码的Semantic Structure（语义结构）要有深度理解才可以。
A: 对对对。用现在的这种Personal Attack的方法，前提就是你得拿到Personal Information，然后再去攻击，我觉得这个是反的。你应该从密码里抓出个人信息，然后再用这个继续去解其他东西，对吧？而不是说你已经抓到了这个人信息，就已经非常Targeted了。
B: 是的。
A: OK，Anyway，这个就是我个人的判断。那么如果要是说C/B的话，你现在有没有什么具体的发给？
B: 我最近还没有看，现在还不知道。就是我这两天也看一下吧，看看我们最好是能在这个3月份、4月份左右，看看能不能先投出去。我给你发一个页面，这个页面我之前好像没跟你讲过，但是这个是我过年的时候做了一下，这个还是挺有意思。你等会儿啊……哎，那个SOX好像是4月22号截稿，这个我感觉倒是可以。
A: SOR……OK，你觉得这个S...应该能上吗？它当然是C/B类，但它是B里边比较难的。因为我看最近好像也没有什么其他的比较合适的。这个……嗯。
A: 嗯，那个RAID的那个单子呢，应该是在4月份。RAID好像还没出来，对我这里好像看到好像还没出来。现在是应该是在4月份，应该早年都是去年的4月份。

A: 嗯，他这个因为我……我这有一个……我这里也有一个网站叫那个CCFDDL.com，就是CCF deadline.com。他那里面有，然后我选了那个“网络与信息安全”的。我看了一下，好像比较合适的也就是ACSAC，4月22号的。就是目前他是把那个已经列出来的那个deadline放下去了。对，他因为是那个……因为那个因为没出来嘛。

A: 对对对，如果加上人工智能的话……但人工智能好像……但是你这个东西跟人工智能关系有点弱，所以人工智能人不一定感兴趣，说实话。

B: 对对对，是的是的。

A: 稍微等会啊，我先看……嗯，是，这个这个啊就是……OK这个部分这……嗯，还是C出了。嗯，保留我之前的。还有个error好像是没有……是像的。嗯，证据是……嗯，嗯，看来我这个程序有点这个错误，这个怎么变成这个？嗯，我之前用的是这个，发在聊天里了。

B: 嗯，是。OK，这个反正是对的了。

A: 这个是对了的话，OK，换一个链接啊。嗯，这个这个你们也可以考虑用一下这个。我……我这个元旦这个假期做完之后，我其实自己一直在……AB也在改这个，主要是方便我自己。但是，嗯，大家……这个你之前可能也知道，就是那个我自己把我的那个我自己网站上有一个CFP papers那个list，手工……之前是手工维护的，所以然后经常没时间，所以就崩退掉。后来我这个圣诞节期间，我我用那个……基于那个整个东西全部给那个自动化了。

A: 嗯嗯，自动化了之后呢，现在是后台有1个Excel文件。这个Excel文件的话，呃，我只要维护这Excel文件就可以了。这个Excel文件才能不能自动转换成这个几个大列表，所有的大的都是Run……呃都是Run。然后我把那个CCF、呃那个……澳大利亚的那个CORE，还有就是那个中国的CAS的那个ranking也加进去了。

A: 嗯，然后呢我还加了一个……在那个表的表头里还加了一个filter。你现在……我现在发给你呢，就是filter里边呢，用了那个Regular Expression（正则表达式）。现在的这个Regular Expression列的是什么呢？列的就是Security方面的会议的CFP。嗯嗯嗯嗯，你如果要是看一下那个……我给你说了一下那个……

B: 确认错了一个是吧？

A: 嗯，对……理……嗯，不对不对，这个是……老虎老虎……这再这这这个对啊。你看这个能看到我的屏幕吧？

B: 嗯，可以可以。

A: 你看就是这。这个来说，上面这里就是前面的这个部分呢，就是一堆呃关键词。关键词哦，看到了，你这个可以改。然后后面的呢，呃，后面的这个呃这个如果你熟悉Excel的话，你知道这个呃一个括号加加叫等号，这个就是AND。就就这就是AND，就是说前面是一个AND，后面是一个AND。然后呢，这个方就是CCF equals to B。为什么呢？就是因为如果你看一下，呃，比如像这里这个ACSAC对吧，他的rank呢就是写的是A，然后CCF B。他那个其实就是匹配这个怎么算。

A: 嗯，然后呢那些所有的那些关键词呢，就是匹配这里边所有的那些关键词。如果有一个能匹上，那么这个就会列出来。所以你能看到在这个400个conference里边的话，呃，目前单子呢还在有的，符合这个条件呢是一个位都没有的。那么但是的话RAID在这儿，这个deadline还没出来；然后ACSAC在这，这个deadline也还没出来。但是呢他现在显示是去年的deadline，已经能看到去年这个ACSAC是……那么这个对……对于这个RAID就是这个4月份。

A: 嗯，所以就是说如果说要做规划的话，那么这里没有，那么那就是看这里了。这个就说这两个表的区别呢，就是你可能看同海……这个是deadline仍然是open；这个中间这个是……呃不是，这是deadline open；中间那个是deadline to be decided；那么下面的是deadline已经passed，就是去年。

A: 嗯，那么这些数据我等他们出来之后，我看到都会改。改了之后他就会给他拉到……拉到前面去。就是如果说比如说RAID deadline出来之后，他就会出现在这个表格里面，就第一个表格里面，然后他会让出现排序。

A: 那么，所以这里就是说，然后你可以做的事情还很多了。你你可以如果要想要看中国的，也可以把……啊这里没有中国的。呃你要做USENIX，我觉得是用了两个USENIX的回复。嗯，这这这这这个用起来应该说目前还没有，我自己还没有看到有有系统能做这么这么多的事情。

A: 然后除了这里这里边的话，你会发现有一些会议是不在里边的。呃，这个我还没整合进来，我目前是放在这个……放在这个呃页面里的。这个页面放的是那种有多个deadline啊，有多个date，你就能看到顶会都在这儿。嗯嗯，包括CCS，也要子，所以你就能看到他的date在这。然后包括DSN，DSN的date，但DSN已经过了……

B: 我们这玩意是不是Industry呢？好像也算Industry。

A: 呃，他这边的好像有一定要求，就是比如说可能要Industry track呀……

B: 对的，他这个是不是得有一些就比如具体的应用啊，在关键应用啊是不？

A: 所以就是说他你可以看一下下。他要他实就是他就是几个要求，刚才就是要求确实要求都是要么就是业界的应用——你可以只是Researcher，要么就是有的一定是高……就他肯定还会有一定的这个问题出来，可以看一下他。嗯，你看他那边就有Case的，就是这样。

B: 嗯嗯，Operational challenge对solutions……

A: 嗯，但是可能不是非常合适，我觉得因为我们这个更多的还是比较……嗯，OK这个反正就是大家就这样，就是说好。然后我目前的计划呢，是准备把这个……啊，当然还有……他点也基本上过了。嗯，不对这个deadline错我改一把……这个进入单是……哦，对，这是确实没错。嗯，ACSAC……CCF B其实选择不是非常多。

B: 是手吧对吧？对对。B的话现在看起好像就是那个ACSAC，那个RAID是……RAID也是B，对吧？刚才看到。

A: 呃，好像是的，好像是的。就刚才我那个……刚才我这个地方显示的是B，它就是B。

B: 对对对。

A: 那就是说如果是要B的话那，基本上就是RAID和那个ACSAC，还有啊……ACSAC。嗯就是4月份啊，应该也是……我们现在可以看一下他那个20XX出来那个……当就说因为但是这种会实际上都是实际上都是A减，就是实际上他们都是都是比较难的。所以就说目前来这个结果如果不能，还是要解还要好好解释一下，然后要做一下那个那个difference的那个explanation之类的。

A: 然后嗯嗯，然后paper……Paper肯定要要要想一些办法去Answer一下。这个工作王静他们还有可能是Reviewer，所以就是最后，嗯，还还还还要注意一下措辞，对吧？还不能多的，就是不给面子或是怎么样。

B: 嗯嗯，对。这个这个s的出不来，这个搜银可能搜到进……
A: 这个USENIX道理应该是应该放出来了，把他一直都没有，一直都放出。啊，2026出来了，OK，2026出来了。哦，这就是在那个王维志他们……王维，应该是王维志他们的Boss。嗯，哦好像还没有，还是特别多……这没关系的，至少网站出来了。
A: 对啊，应该估计这也快了。应该是毛文琪他们在不在？对，不来。好，这边把……对，这个王老师是到处搞三联，你们知道啥是吧？邱老师好像提到过，但我不太认识他。反正他还有那个谁，还有梁，都是属于是狂办会议的人啊。当然他们也会办上一些比较大的会，比如梁开泰搞了那个什么，他其他顶会都搞完，能搞的都搞完是吧，然后今年还要那个SET，好像也是他们主管。
A: 哦，嗯，等着我把这个USENIX再去改一下。嗯，是2026年，嗯对不对？Double blind，然后这是10月11日。
B: 对，因为他这个比……OK，他这个日期其实跟那个S&P差不多，所以我们本来如果要投的话，肯定要考虑一下投哪一个。嗯，我们现在应该是就先以S&P那个deadline做准备呗。然后因为他那个时间我估计相差也无几，然后到后面我们看看春稿之后的这个效果，然后我们再去决定投那个都行。
A: OK，那就按照S&P准备。然后呃，主要的目的是要把呃，这个“一步（One-step）”的rule的这个解释要做好。然后嗯，如果能做好的话，那么，那么的话我们就有可能，就就能够明确的就是解释出来的主要的好处，就是说我们就知道之前的方法那个本质缺点在哪里。对，就它看起来很好，为什么连“一步”的都做不好呢？然后这里面就会有很多Insights，这个的话就会变成亮点了。就是说虽然我们的方案也好像也没有什么出彩地方，但是这个Substantial difference of the performance，还有这个Analysis，show us how much the old……对吧，let have missed。然后当然如果这个东西是不是代表Paper miss，那who knows。
B: 哦，他这里其实刚才我也在考虑。就是虽然说我们说的是“一步”，但是他这个“一步”它所能代表的含义其实还蛮多的。就是他这个里面……就比如说他添加的话，他也有可能会说就是到底是比如说头部添加、尾部添加、添加在哪个位置上、添加什么、包括添加是什么字母啊。
A: 对，但但是……但还是那个老问题。这个东西，你用最难用的方法就可以抓出来的，就是Edit Distance（编辑距离）呀。就是呃，我们其实就是HHH3设……就是说，但这时候你们的传统方法连这么基本的东西都做不到，就这个意思。
B: 嗯嗯，啊这个这个确实，这个后面我们也去比较一下，那到底是哪里是这个有差异。我觉得这个因为……就因为你想如果是这样，如果说那个，呃，我们假设他说多步的其实并没有做到啥，多步的实际上意义不大。如果多步的意义不大，也就意味着，呃，包括我们的方法也是没意义的。为什么呢？你就用完个……你就把Stat做完，把那个概率硬算出来，然后去做一个非常Naive的model就可以了。That's that's all need。也就意味着这个研究吭哧吭哧了半天，其实就是还不如用一个最简单的方法，就是直接把东西穷举出来就完了。Chat完毕，那就说就打脸了，对吧？就是把简单问题……
B: 他这里也不是，就是说因为我们呢，是是就是那个什么意思，就是我们也不会说把所有东西全枚举出来，因为我们要是在这个有限的尝试之内。就是他这个概率的排序其实还是有说法的。有限的尝试是什么意思？就是比如说那个，比如说我拿到了一个口令之后，然后呢，我要去针对它去生成一些这个呃，比如说Top N的这样的一个规则List。
B: 那就比如说“一步”的规则，我也会生成出那个就是前……比如说啊，前20个规则，然后把它应用在这个口令上。它其实是很难通过就是把所有的东西列举出来这样子。那如果是统计的话呢，倒是也有可能。就是相当于说，我要把所有的那个这个“一步”的我都统计出来，然后我排个序。那我以后这个其实就有点类似于那个我们教那个……就是Hashcat，他自己也有一个叫做那个Best64，也就是Base64啊，就有个这样的一个Rule。这个我们也比较了。
A: 呃，我没有非非常明白你的意思。就是如果你用穷举的方法，就是用A2那种方法，直接把Ground Truth其实就是把“广柱子”算出来。因为算Ground Truth本来很简单，不需要再套一个LLM再去重新再Predict Ground Truth。你都已经用Ground Truth还Truth不是？
B: 是这样的，就是核心的呢，是我们其实是……比如说我拿到了一个口令之后，然后呢我要去生成出它相应的规则嘛。就是相当于说，呃，ASR算法它是有要有一个Pair，它才能去输成出来嘛，就相当于是那个算它的这个具体的这个Edit Distance嘛。然后呢，我们能做的呢，就是说比如说我已经有这个训练集了，那我把所有的这个训练集的这个“一步”的策略我全算出来。然后呢，我就可以去……我什么都不做，我就只是去算一下它的这个出现的这个概率。然后我比如说我只写100个，肯定呃，取前100个规则这样子。就这个可能是传统的那个，就是那个编辑距离的这样的做法。
A: 对。是LLM的做法改进了什么呢？
B: LLM的做法呢，实际上就是说，呃，我其实就……就是这些呢实际实际上作为我的训练集了嘛。就是呃，编辑距离我能做的就是我把它统计一下，然后呢我把它这个取前20个，比如说这样子。然后呢，但是这个东西到底适不适用，其实我也不知道。然后LLM呢，或者说那个其他这个方法学的，他们的目的其实是：当我输入了，比如说我输入了1000个这个规则之后，然后他把一个……他拿到这个口令之后，我会针对这个口令来进行一个相应的一个规则的一个排序。然后我针对这个口令，我去生成出一些这个规则。相当于每个口令里，它的前20个规则可能都不一样。这样它的这个多样性就增加了。然后这样的话呢，也能证明说这个也就是为什么我们比其他的东西……可比其他的这个方法可能要好一些，可能会好在这里。
A: 就我说目前的口令是以前训练的时候看到的，是没看到的？
B: 哦，没看到的呀，没看到的，没看到的。
A: 所以说你……对，就说你是用了200万这个Pair，生成了一个Password Dependent的那个Rule Deliver？对，然后用这个来去训练一个LLM。然后这个LLM的话，会对那种没有见到过的密码，对，在推测它的可能的变换？
B: 对对对，对对，没错。
A: 那么这就是为什么统计用做不了，因为他不能做外推。
B: 对对对，对对，是的。
A: 那么也就是说你实际上在做性能分析的时候，你必须得至少得有三个DB吧，把分三个……分给4个，你用两个来做训练，用另外两个做分析。
A：是这意思吗？
B：呃，我们目前的做法就是，比如说我在两个库当中选了100万个，然后我用比如说30%做训练，另外70%做验证。大概就是对同一个库嘛。
A：对对对，对同一个库。那不是就有那个Bug？
B：呃，可能会有，但是当然我们后面也可以去尝试，就是分成不同度，然后进行一个比较，我觉得这个也可以的。
A：OK，就是说等于说你上这个复杂的方法的主要目的，就是为了处理Unseen（未见过的密码），是这样对对？如果你要Already Know Password（已经知道密码），你直接从那个Ground Truth（真实值）里边来抓就行了，因为你Ground...
B：对对对，没错没错，是的。因为他肯定要针对那个没见到过的、没见过的这个最Pooling（普遍/汇聚）的。
A：但那样就是说，如果你碰到了见到的，你会直接抓吗？还是说还是用Model跑卡（模型预测）？
B：呃，还是依赖于模型了。这个我们都依赖于模型的输出了，不会说直接去发现找到了我们就直接输出了，就看模型它是怎么去规定。
A：那也就是说理论上有可能你的这个方法还不如那个用这种最笨的（方法）？
B：呃，有这个是有可能的。所以说我们当时呢也用了另外一个比较，就是张连浩没在这个里面展示出来，其实他还跟我们也做了一个对比，就是那个Hascat（HashCat）。
A：嗯。
B：他有一个叫Best66的这么一个Rule（规则），号称是从以往他们经历过的所有的这些规则当中，总结出来这个66个最好的。那这个我觉得其实就可以作为一个基于统计学的Baseline（基准）了。
A：然后我这个最好的问题是，它还是Pass（规则匹配），对吧？
B：嗯，对对。
A：我的想法是，哪怕你有Naive（天真/简单）的方法，仍然可以比那个做的更好。这个到（时候）我们也可以做一下试试，比如说如果是Naive的方法，像我刚才说的那个Parametrization（参数化）的话就做Classification（分类）。就是你可以用简单的这种Classification的方法，你没见过这个...你如果见过，就直接从那个Ground Truth里抓；你没见过，你无非就是用那个Similarity Measure（相似度度量），如果它和某一个密码比较接近，你就用它类似（的），你就套它。其实也是一种Classification，只不过是比较简单。但是仍然是某种方法吧？
B：但是如果是这种的话，可能有一个问题。就是比如说我的一个口令，它是在我训练集当中只出现过了一次，比如说我Ground Truth里面只出现了一次，那这个时候如果我只是在Ground Truth里面去抓的话，那它就很难有这种扩展性。就是比如说我要生成20个，那它就很难去（生成）其他的几个，它其实还是要依赖着它生成的。
A：OK，就不管怎么说吧，反正就说这个应该是...一个就说文献里边对于这种比较Simple的这种，但是他们都没有去建实际的、更Smarter的这种Bench（基准）对吧？
B：嗯，好像并没有。他们基本上也都是生成出规则之后，然后拿着那个新生成的规则去做Pass（破解）的，都是这种。
A：OK，那也就是说，因为最主要的问题就是说，你完全用那个Gla（生成模型）结果的最大的缺点，就是完全忽略了Ground Truth。完全忽略Ground Truth的最大缺点就是说，原则上来讲你应该是没有Ground Truth好的。你把它完全忽略之后呢，你带来的好处就是你没见过的密码你也怎么处理；坏处就是你那些本来能够处理的更好的、能看到的密码，你现在也变成是猜测。
B：呃，所以这也不一定。就是说，比如说那个虽然我的PW（密码）我见过，但是我的Payload（或指变体）有可能是没见过的。就是比如说我一个人在三个网站上有口令的话，然后我去切分的时候，我只切分到了他的两个网站，那他就相当于是他本来有两个PW，他只出现了一个PW嘛。然后我自己做确定的时候，我会认为就比如说30%这一部分是这个Ground Truth。
A：OK，就是说实际上是Old和New都有放的情况，所以是两块都会对（有影响）。
B：对，所以的话如果是用数字就不会太难用。
A：OK，这个的话好像更合理一些。
B：对。
A：OK，那就大概这样的话，我在想如果是Single Rule（单条规则），也就是说LM（语言模型）可以更好的预测Single Rule，其他的方法可能没有预测出来。比如说像你刚才讲的，如果是Password123改成Password124、Password125，那么他们可能只看到Password124，因为Password124想跑（出来）。但是用LM的话，它就可以做更General（通用）的这种Extension（扩展），包括通过看其他用户是怎么做的，对吧？
B：对啊，就做更多的这种三个（层面）整合。
A：对对，但这个应该是可以通过那个Diff Set（不同数据集）的那个分析能够体现，对吧？
B：对对对，我觉得可以是两方面去验证一下，我觉得可以的。
A：OK，如果是这样的话，也就意味着加上Dataset这个分析，然后我们有一定定性的解释了。那么剩下的可能主要的问题就是Pass（通过率/相关工作）。然后画出来的话，就要去思考怎么样去Argue（论证）。那个（论文）好像是哪一年发的？好像也挺近的，23年还是24年吧？
B：哦，那就不近了，两年都是很很久的过去了。在这个地方，在（安全）领域就是...
A：Asian的H（AsianCCS）？
B：对，确实，对的。
A：哦，那个好像是23年，23年的US（USENIX）。
B：对，23年。哦对，应该是和PassGPT是一年。
A：对，等于是23年之后没有新的进展了。如果有其他的比他更新的、号称比他更好的，我们可以把它Pass（超越/击败）。有这样的吗？
B：目前好像没看到。
A：哎，我们再去再去看一下那个Pass2Edit...那个呃不，Pass2Pass。Pass2Pass是哪年的？它有没有和Edit（Distance）做比较？我有点忘了，我们再找找文献吧。
B：Password...
A：哎，PassLM它有和那个Pass2Parent（或类似模型）的比较吗？我有点记不清了，Benchmark the price（或者是Benchmark Comparison）什么...没有钱呢？
B：哦，既是关...
A：哎，我这就有个Point，那个Pointer Guess（指针猜测）是2024年，Pointer Guess你们没有看吗？
B：Pointer好久没有...哎，刚修店（USENIX）也是他们指的...Ground Truth就是他们所最喜（欢的）对吧？
A：哎，这个好像还真记不记了，这个也不记得吧？那该得...14年就是他们在发拉（会议）发出来的最后那一年发。我可能...我哎好像有点印象，但是我记得好像和我们这个关系不是特别大。就是说他可能那个...就是他什么都是Guess，有点记不清了。
A: 然后呢，还有就是他的那个LM也是做这件事情嘛，你说的对吧？他有部分做了这件事情。
B: 对。
A: 就是说，他那个工作你是说是有代码的对吧？
B: 对。
A: 那就是说，那个是不是可以拿来做一个新的baseline，就可以把base的这个删掉了？好像可以哎，因为毕竟是同一个组的东西嘛，而且那个更新嘛，对吧？他这个是2025年的，就是说他们组的东西，最新的就是这个PaLM，次新的就是这个Pointer-Gen，呃，都是在USENIX degree。那你如果说要是要……但是有一个问题就是，PassLM它是有这个额外的输入的，我估计PassEdit也应该也是有这个额外的personal information的输入的。就是这个原因的话，你可以用这个原因把它删掉；但如果把这个没有原因把它删掉的话，就是说就会变得比较窄一点。我看看啊，first model the password tagging as a multi-class problem……对，哦，嗯，PassPark太老了，PassPark是2019年的，所以学用PassPark那不行。但是新一点的，这个我再去看一下那个Pointer-Gen和那个什么。啊，因为2019这个都是7年了，太老。最好是能够把他的那个PassLM拿来做比较，这样的话我们……PassLM他反正他是他源码都有嘛。还有一种做法，就是我们就说我为了对齐，我们就不把个人信息输入进去，我就直接去拿他的那个……就直接相当于是只拿口令这块去做，这样反正也行。
A: 对，还有一种做法就是你把personal data也放在我们的训练里面，这是另外一种对齐方。一般我们和他们对接，和他和我们对齐，这是两种对接的结果对焦。
B: 对对，行啊，那这个我觉得这个也可以操作，我们可以试试。因为如果这样的话，你的那个argument就会比较简单。Argument就是说：for one group we use their latest model，这样的话直接把那个PaLM和那个换上，直接就全都全bypass，因为这些都是Open的吧？
A: 对，好的呀。然后这样的话，因为如果是S&P或者是USENIX的话，都是两个多月的时间，时间可能也还是应该够。因为有代码就能跑，对吧？
B: 对。
A: OK，那就按照这个思路走一走。然后我的想法就是如果能做完，那就……我的建议是当然如果是……如果这样的话，对这个我们回来做LM、改进这个SP3季有兴趣的话，那么是不是还是转到这边来会更有意义一些？
B: 嗯嗯，好的呀，嗯。
A: OK好的，那今天要不就先这样？
B: 好的。呃，比如说还有一件事情，就是那个PassFinity那一块的话，也给你同步一下。就是您不是把那个修改后的那个发给我了吗？然后我去问了那个学院那边，学院这块说就是他们要去确认一下都改在哪个地方，然后也要让那个……啊，我再看一下，因为那个之前那个审批的是相当于是我们电院嘛，但是后来我们计算机学院即是不是拆分了？就我们电院拆分了嘛。
A: 对对对，拆分了，搞定了，在单商了。
B: 对，就是电院拆分了4个学院，现在是这样子，然后那个……所以说他们可能还是重新审一下，但是这个因为他现在在寒假常，可能稍微晚一点。反正这个到时候我去更新，那个如果有这个情况，我会跟您说。他就说好像是最后那个有一个段落，就是用来呃……说那个语言的，就是说英语。
A: 对，我看到了，我看到了。那个应该就是没有个别的，应该就是那个没有别。我跟他们也说了，他们说他们再确认一下。我觉得这个也无可厚非嘛，毕竟那个有改的，让他们再去看呗。
B: 对对。
A: 就说你反正告诉他们，就是说文本是没有改动的，本来就是改成了这个语言的。因为你中文……因为它主要是有中文在，所以他调重庆就说中文其实是不算数的，中文只做参考。
B: 行，好的好的，行。然后那个另外是那个……我也问了一下那个卢硕，就是之前我们发C的那个呃……论的是那个文件的话，他说那个他是上次那个袁跟他提了一些修改意见之后，他已经改……嗯，那个袁海月了。就是这块那个，我不知道那个后来有那个东西，如果我没记错的话，应该是卡在我这了，因为我要看一下。后来也一直没看，我现在看到找找算了啊。
A: 好的好的好的，就这个查一下啊。对，这个……那这个因为就是说那个……对，这个好像也不妨碍我们这个LaTeX的东西，对吧？没有LaTeX东西也可以对做对不管这件事情。呃，因为这个特别是时间太多，这个没有放上这个日程上。我等一下，我来找一找啊。
B: 行行行，没问题，不急，我跟自己同步一下就行。
A: 嗯，OK好的。嗯，行，其他那个……其他的应该没什么了。其他这个然后我的那块的话，现在这两天我那个项目弄完了之后，我会再继续推进，再去做一下。
B: OK好的。嗯，然后……然后那个李教授，我们下周是就要到春节了，然后那我们再开会的话，要不看看可能就到3月1号了？
A: 没关系，这可以的，就春节后呗。我我和那个另外一个组也是类似的，也是三月一号。
B: 好的好的好的好的，行，那等到时候如果我这块有一些比较重大的一些进展，或者需要跟您讨论的，到时候我再联系您。
A: OK。我的这个LaTeX好像有点找不到了，我都不知道放哪了，我就问一下阿里云……这个我有印象，但是但是我不知道放哪。啊，他们应该都是邮件，那个如果您找不到的话，到时候我问一下卢硕，看他那有没有保存副本也行。
A: 对，最主要是那个袁海月可能改过一次，所以他改过一次了之后……哦，他放哪了？我现在因为时间有点久远了，我又记不太清楚，问一下吧。
B: 好好好好的好的好的，没问题，没问题。
A: OK好，行，那那就先这样。OK，那就春节愉快，我们今后再见啊。
B: 嗯对，谢谢教授，元宵节愉快。
A: 嗯哎，张林浩，再见。
B: 嗯，以后再见。
（通话结束，B转向他人或自言自语）
B: 哦，喂嗯，去啊。哎哎，张林浩……行，那个别的应该也就没啥。就是那个我们的个目标可能会放在就是4月份的那个……呃，目前啊我们暂时暂定的就是那个4月份那个SRX，还有两个多月的这个时间，我就可以把现在的这个整理整理，然后那个可以看看就是能不能写一篇文章，到时候先投到那边去。然后呢，至于说那个李教授，他说的那个就是……那个训练场就是那个ground truth——他叫ground truth，我们可能叫做这个training set嘛，就是那个训练集这一块——觉得也可以确实看一下。就是我们跟他们的那个……他，我记得他们那个应该是，呃，那个模型应该也是用我们的那个训练集重新训练它这个结果，对吗？呃，就是那个Pass-to-Pass他们吗？对对对啊，是的是的。OK，那我觉得这块可能也没什么特别好去讨论的，我觉得可能就是我刚才像我说的……
A: 就是我们在那个……就是一步的这个可能就是会更好一点。然后那个 PasLM 的话，我觉得可以看一下。就是他们对于那个……因为我刚才翻了一下那个 PasLM 那个文章啊，他也用了那个……也有和 Pas Edit 的比较嘛。那也可以看一下，就是他们那块是怎么做的，我们能不能就把我们的那个训练集在那个 Pas 上去实现一下，试试。
B: 嗯，好的，嗯。
A: 然后别的应该也就没啥了。然后我们下一周也就不开会了，就是一直到那个春节之后吧。到时候……哎，我们是几号开学来的？
B: 3月1号开学。
A: 啊，对对，3月1号开学。啊，OK OK。那就等开学之后，到时候我们再组那个主持会议嘛。然后那个你这块如果有一些比较具体的进展的话，然后有需要讨论的、有些问题需要讨论的话，到时候你可以随时联系我。我这个假期应该都在上海。
B: 嗯，好好。
A: 就说那个你如果有需要讨论的，你就直接联系我就好了。
B: 嗯，好的好的。
A: 嗯，好，行，别的就没啥了，那就先这样。
B: 行，那学校再见啊。
A: 嗯，哎好嘞，拜拜。